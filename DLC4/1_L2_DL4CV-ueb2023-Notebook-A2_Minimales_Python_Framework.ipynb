{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 2 - Anwendung auf Regressions- und Klassifikationsproblem\n",
    "\n",
    "Dieses Notebook thematisiert die Umsetzung einer automatischen Differentiation, die die Grundlage vieler Deep-Learning-Frameworks bildet.\n",
    "\n",
    "Ziel ist es, zu verstehen, wie eine automatische Differentiation funktioniert, welche Vorteile sie bietet und was bei deren Verwendung beachtet werden muss.\n",
    "\n",
    "Füllen Sie die Lücken enstprechend und speichern Sie das ausgeführte Notebook erneut ab (File &rarr; Download as &rarr; Notebook). Reichen Sie abschließend das heruntergeladene Notebook im zugehörigen [Moodle-Kurs](https://moodle.tu-ilmenau.de/course/view.php?id=28) ein.\n",
    "\n",
    "**Die Einreichungsfrist finden Sie im Moodle-Kurs.**\n",
    "\n",
    "### Inhaltsverzeichnis\n",
    "- (h) [Implementierung eines minimalen Frameworks](#minimales_framework)\n",
    "    - [ I ] - [Die Klasse Value](#h_i)\n",
    "    - [ II ] - [Berechnung mit der Klasse Value](#h_ii)\n",
    "    - [ III ] - [Partielle Ableitung mit der Klasse Value](#h_iii)\n",
    "    - [ IV ] - [Die Potenz in der Klasse Value](#h_iv)\n",
    "    - [ V ] - [Berechnungen mit der Potenz](#h_v)\n",
    "    - [ VI ] - [Weitere Funktionen für die Klasse Value](#h_vi)\n",
    "    - [ VII ] - [Berechnung des Gradienten mit der erweiterten Klasse Value](#h_vii)\n",
    "- (j) [Regression einer Geraden](#regression_gerade)\n",
    "- (m) [Regression einer Parabel](#regression_parabel)\n",
    "- (n) [Implementierung eines neuronalen Netzwerks](#implementierung_neuronales_netzwerk)\n",
    "    - [ I ] - [Die Klasse Module](#n_i)\n",
    "    - [ II ] - [Die Klasse Neuron](#n_ii)\n",
    "    - [ III ] - [Implementierung einer vollverschalteten Schicht](#n_iii)\n",
    "    - [ IV ] - [Implementierung von ReLU und Sigmoid](#n_iv)\n",
    "    - [ V ] - [Der Digit-Datensatz und Datenvorverarbeitung](#n_v)\n",
    "    - [ VI ] - [Erstellung eines zweischichtigen MLP](#n_vi)\n",
    "    - [ VII ] - [Implementierung des Trainings](#n_vii)\n",
    "    - [ VIII ] - [Implementierung der Validierung](#n_viii)\n",
    "    - [ IX ] - [Training und Validierung](#n_ix)\n",
    "    - [ X ] - [Visualisierung von Loss und Accuracy](#n_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-width: 5px\">\n",
    "\n",
    "### Vorbereitung\n",
    "Wichtige Ergebnisse können während der Bearbeitung überprüft werden. Grundvoraussetzung hierfür ist, dass Sie das Paket `tui-dl4cv` <font color=\"#aa0000\">installieren bzw. aktualisieren</font> und anschließend importieren.\n",
    "\n",
    "Für die Installation stehen Ihnen zwei mögliche Wege zur Verfügung.\n",
    "\n",
    "**(1) Installation direkt in diesem Notebook:**\n",
    "Führen Sie den nachfolgenden Code-Block aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(f\"Automatically install package for '{sys.executable}'\")\n",
    "!{sys.executable} -m pip install tui-dl4cv \\\n",
    "    --extra-index-url \"https://2023ws:QSv2EKuu9MmyPAZzez82@nikrgl.informatik.tu-ilmenau.de/api/v4/projects/1730/packages/pypi/simple\" \\\n",
    "    --no-cache --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ODER\n",
    "\n",
    "**(2) Manuelle Installation über die Konsole:**\n",
    "Öffnen Sie eine Konsole (\"Anaconda Prompt\" unter Windows) und führen Sie folgenden Befehl aus:\n",
    "```text\n",
    "pip install tui-dl4cv --extra-index-url \"https://2023ws:QSv2EKuu9MmyPAZzez82@nikrgl.informatik.tu-ilmenau.de/api/v4/projects/1730/packages/pypi/simple\" --no-cache --upgrade\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Führen Sie abschließend folgenden Code-Block aus, um das Paket verwenden zu können.**\n",
    "Während der Bearbeitung können Sie nun Ihre Ergebnisse mithilfe der Funktion `interactive_check` überprüfen. Die Funktionsaufrufe sind bereits an den entsprechenden Stellen im Notebook enthalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tui_dl4cv.framework\n",
    "\n",
    "# noetige Erweiterung, damit Variablen aus diesem Notebook automatisch ueberprueft werden koennen\n",
    "def interactive_check(name, **kwargs):\n",
    "    tui_dl4cv.framework.interactive_check(name, globals(), **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-width: 5px\">\n",
    "\n",
    "<a name=\"minimales_framework\"></a>\n",
    "### (h) Implementieren Sie ein minimales Framework in Python, welches automatische Differentiation unterstützt und setzen Sie folgende Gleichungen um:\n",
    "\n",
    "$y = z \\cdot z$, &emsp; $z = x \\cdot x \\cdot 3 + x \\cdot 2$ &emsp; für &emsp; $x = 5$\n",
    "\n",
    "Berechnen Sie mittels automatischer Differentiation die partielle Ableitung $\\frac{\\partial y}{\\partial x}$ an der Stelle $x=5$.\n",
    "\n",
    "Als Autograd wird die Umsetzung einer Graphkonstruktion zur automatischen Differentiation bezeichnet. Die nachfolgende Implementierung basiert auf [micrograd](https://github.com/karpathy/micrograd) von Andrej Karpathy (Ersteller der Vorlesung \"CS231N Convolutional Neural Networks for Visual Recognition\" der Stanford University).\n",
    "\n",
    "Die Basis für die Knoten des Graphen stellt eine Klasse dar, die einen aktuellen Wert der Berechnung im Forward Pass und die Ableitung im Backward Pass speichern kann.\n",
    "\n",
    "---\n",
    "<a name=\"h_i\"></a>\n",
    "#### [ I ] - Machen Sie sich mit der Implementierung der Klasse `Value` vertraut.\n",
    "\n",
    "**Erläuterungen zur Funktionsweise**:\n",
    "Der in der Klasse gepeicherte Wert kann bei der Initialisierung der Klasse (Funktion `__init__`) übergeben oder berechnet werden.\n",
    "Für den Forward Pass müssen Funktionen für alle unterstützten arithmetischen Operationen bereitgestellt werden, also z.B. Addition (der Operator `+` wird in Python durch die Funktion `__add__` definiert), Multiplikation (der Operator `*` wird in Python durch die Funktion `__mul__` definiert) und Potenz (der Operator `**` wird in Python durch die Funktion `__pow__` definiert).\n",
    "Jede dieser Funktionen muss auch eine Berechnungsvorschrift für den Gradienten enthalten (`def _backward()`).\n",
    "Die Berechnungsvorschrift wird im Klassenmember `self._backward` gespeichert und während des Backward Pass zur Berechnung des Gradienten (Funktion `backward`) aufgerufen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value:\n",
    "    \"\"\"stores a single scalar value and its gradient\"\"\"\n",
    "\n",
    "    def __init__(self, data, _children=()):\n",
    "        self.data = data    # data contains the value of this variable\n",
    "        self.grad = 0    # grad contains the gradient, it must be initialized with zero\n",
    "        # internal variables used for autograd graph construction\n",
    "        self._backward = lambda: None    # initialize backward step with empty function\n",
    "        self._prev = set(_children)    # _prev contains child nodes\n",
    "        self._retain_grad = False    # gradient should not be retained per default\n",
    "\n",
    "    # forward and backward pass for basic operations\n",
    "\n",
    "    def __add__(self, other):\n",
    "        \"\"\"operator + (addition: self + other)\"\"\"\n",
    "\n",
    "        # other needs to be an instance of class Value (self.__class__)\n",
    "        if not isinstance(other, self.__class__):\n",
    "            other = self.__class__(other)\n",
    "\n",
    "        # calculate the sum\n",
    "        forward_result = self.data + other.data\n",
    "\n",
    "        # create a new Value (self.__class__) that can store the result and the calculation tree\n",
    "        out =  self.__class__(data=forward_result, _children=(self, other))\n",
    "\n",
    "        # backward path: both children get same (full) gradient\n",
    "        def _backward():\n",
    "            # calculate gradient and add it to grad:\n",
    "            # adding the gradient is necessary, since a Value can get gradient information\n",
    "            # from multiple other Value nodes and these gradients must sum up\n",
    "            self.grad += 1 * out.grad\n",
    "            other.grad += 1 * out.grad\n",
    "\n",
    "        # add this backward function to the new node\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        \"\"\"operator * (multiplication: self * other)\"\"\"\n",
    "\n",
    "        # other needs to be an instance of class Value (self.__class__)\n",
    "        if not isinstance(other, self.__class__):\n",
    "            other = self.__class__(other)\n",
    "\n",
    "        # calculate the product\n",
    "        forward_result = self.data * other.data\n",
    "\n",
    "        # create a new Value (self.__class__) that can store the result and the calculation tree\n",
    "        out = self.__class__(data=forward_result, _children=(self, other))\n",
    "\n",
    "        # backward path: partial derivative for self and other\n",
    "        def _backward():\n",
    "            # calculate gradient and add it to grad:\n",
    "            # adding the gradient is necessary, since a Value can get gradient information\n",
    "            # from multiple other Value nodes and these gradients must sum up\n",
    "            self.grad += other.data * out.grad\n",
    "            other.grad += self.data * out.grad\n",
    "\n",
    "        # add this backward function to the new node\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "    def _is_leaf(self):\n",
    "        \"\"\"check if the current node is a leaf node that has no children\"\"\"\n",
    "        if not self._prev:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def retain_grad(self, retain_grad=True):\n",
    "        \"\"\"specify whether the gradient should be retained for this variable\"\"\"\n",
    "        self._retain_grad = retain_grad\n",
    "\n",
    "    # gradient descent starting from this node\n",
    "    def backward(self, grad=1):\n",
    "        \"\"\"backpropagates the gradient to all children starting from this node\"\"\"\n",
    "\n",
    "        # topological order of all the children in the graph\n",
    "        topo = []    # remember the order in which the nodes must be processed\n",
    "        visited = set()    # remember nodes that have been visited already\n",
    "\n",
    "        # breadth-first search to find the processing order of the nodes in the graph\n",
    "        def build_topo(v):\n",
    "            if v not in visited:    # if the node is not visited yet\n",
    "                visited.add(v)    # remember that it is visited now and should not be visited once more\n",
    "                for child in v._prev:    # for each child node of this node\n",
    "                    build_topo(child)    # visit each child node\n",
    "                topo.append(v)    # insert this node at the end of the list of processing order\n",
    "                # (please notice, that all child nodes will be inserted earlier)\n",
    "\n",
    "        # start the breadth-first search from this node\n",
    "        build_topo(self)    # (please notice, that this node will be inserted as last element of the processing list)\n",
    "\n",
    "        # initialize the gradient of this node with the given value\n",
    "        self.grad = grad\n",
    "\n",
    "        # go one variable at a time and apply the chain rule to get its gradient\n",
    "        for v in reversed(topo):    # in reverse order of the processing list that was build above\n",
    "            v._backward()    # call the _backward function that is specified for each operation\n",
    "            # reset the gradient for non-leaf nodes unless explicitly specified\n",
    "            if not (v._is_leaf() or v._retain_grad):\n",
    "                v.grad = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Funktionsweise</b>:\n",
    "\n",
    "Die Klasse `Value` repräsentiert einen Knoten in einem Berechnungsgraphen.\n",
    "In diesem Knoten ist die Berechnung der entsprechenden arithmetischen Operation umgesetzt, sowie deren Ableitung.\n",
    "Außerdem werden die Operanden für diese Operation als Kind-Knoten gespeichert.\n",
    "So lässt sich ein entsprechender Berechnungsgraph aufbauen.\n",
    "\n",
    "Wird die Funktion `backward` aufgerufen, so wird der Gradient dieses Knotens entsprechend der jeweils spezifizierten Ableitungsregeln an alle Kind-Knoten weiterpropagiert."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"h_ii\"></a>\n",
    "#### [ II ] - Wenden Sie die Klasse `Value` an, um folgende Berechnungen durchzuführen:\n",
    "\n",
    "$z = x \\cdot x \\cdot 3 + x \\cdot 2 \\quad$ für $x = 5$\n",
    "\n",
    "$y = z \\cdot z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Value(5)\n",
    "\n",
    "z =    # bitte Code ergaenzen <---------------- [Luecke (1)]\n",
    "y =    # bitte Code ergaenzen <---------------- [Luecke (2)]\n",
    "\n",
    "print(f'z = {z.data}')\n",
    "print(f'y = {y.data}')\n",
    "\n",
    "# Implementierung ueberpruefen\n",
    "interactive_check('II')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>&#9432; <i>Überprüfung &nbsp; &nbsp; <font color=\"CCCCCC\">(anklicken, um Lösung anzuzeigen)</font></i></summary>\n",
    "    $z = 5^2 \\cdot 3 + 5 \\cdot 2 = 85$<br> $y = 85^2 = 7225$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"h_iii\"></a>\n",
    "#### [ III ] - Wenden Sie die Klasse `Value` an, um die folgenden partiellen Ableitungen zu berechnen:\n",
    "\n",
    "$\\frac{dL}{dz} = ? \\quad \\frac{dL}{dx} = ? \\quad$ mit $\\frac{dL}{dy} = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dLdy = 1\n",
    "\n",
    "# retain gradient for z\n",
    "z.retain_grad()\n",
    "\n",
    "# reset gradients\n",
    "x.grad = 0\n",
    "z.grad = 0\n",
    "\n",
    "# calculate all gradients\n",
    "y.backward(grad=dLdy)\n",
    "\n",
    "print(f'dL/dz = {z.grad}')\n",
    "print(f'dL/dx = {x.grad}')\n",
    "\n",
    "# Implementierung ueberpruefen\n",
    "interactive_check('III')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vollziehen Sie die Rechnung per Hand nach und überprüfen Sie, ob Sie auf dasselbe Ergebnis kommen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>&#9432; <i>Überprüfung &nbsp; &nbsp; <font color=\"CCCCCC\">(anklicken, um Lösung anzuzeigen)</font></i></summary>\n",
    "    <br>$\\frac{dL}{dz} = \\frac{dL}{dy} \\frac{dy}{dz} = \\frac{dL}{dy} \\cdot \\left[ \\frac{d}{dz} z^2 \\right] = \\frac{dL}{dy} \\cdot \\left[ 2 \\cdot z \\right] = 1 \\cdot \\left[ 2 \\cdot 85 \\right] = 170$\n",
    "    <br>$\\frac{dL}{dx} = \\frac{dL}{dz} \\frac{dz}{dx} = \\frac{dL}{dz} \\cdot \\left[ \\frac{d}{dx} \\left( 3 \\cdot x^2 + 2 \\cdot x \\right) \\right] = \\frac{dL}{dz} \\cdot \\left[ 6 \\cdot x + 2 \\right] = 170 \\cdot \\left[ 6 \\cdot 5 + 2 \\right] = 5440$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"h_iv\"></a>\n",
    "#### [ IV ] - Ergänzen Sie die Funktion zur Berechung der Potenz und die entsprechende Ableitung.\n",
    "\n",
    "<b>Hinweis</b>: Leiten Sie von der Klasse `Value` ab und ergänzen Sie die neue Operation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value(Value):\n",
    "    def __pow__(self, other):\n",
    "        \"\"\"operator ** (power: self ** other)\"\"\"\n",
    "        assert isinstance(other, (int, float)), 'only supporting int/float powers for now'\n",
    "\n",
    "        # calculate the product\n",
    "        forward_result =    # bitte Code ergaenzen <---------------- [Luecke (3)]\n",
    "\n",
    "        # create a new Value (self.__class__) that can store the result and the calculation tree\n",
    "        out = self.__class__(data=forward_result, _children=(self,))\n",
    "\n",
    "        # backward path: deviation\n",
    "        def _backward():\n",
    "            self.grad +=    # bitte Code ergaenzen <---------------- [Luecke (4)]\n",
    "\n",
    "        # add this backward function to the new node\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"h_v\"></a>\n",
    "#### [ V ] - Vollziehen Sie die Berechnungen aus den Teilaufgaben [ II ] und [ III ] unter Verwendung von Potenzen nach.\n",
    "\n",
    "$z = x^2 \\cdot 3 + x \\cdot 2 \\quad$ für $x = 5$\n",
    "\n",
    "$y = z^2$\n",
    "\n",
    "$\\frac{dL}{dz} = ? \\quad \\frac{dL}{dx} = ? \\quad$ mit $\\frac{dL}{dy} = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Value(5)\n",
    "\n",
    "z =    # bitte Code ergaenzen <---------------- [Luecke (5)]\n",
    "y =    # bitte Code ergaenzen <---------------- [Luecke (6)]\n",
    "\n",
    "print(f'z = {z.data}')\n",
    "print(f'y = {y.data}')\n",
    "\n",
    "dLdy = 1\n",
    "\n",
    "# retain gradient for z\n",
    "z.retain_grad()\n",
    "\n",
    "# reset gradients\n",
    "x.grad =    # bitte Code ergaenzen <---------------- [Luecke (7)]\n",
    "z.grad =    # bitte Code ergaenzen <---------------- [Luecke (8)]\n",
    "\n",
    "# calculate all gradients\n",
    "y.backward(grad=dLdy)\n",
    "\n",
    "print(f'dL/dz = {z.grad}')\n",
    "print(f'dL/dx = {x.grad}')\n",
    "\n",
    "# Implementierung ueberpruefen\n",
    "interactive_check('V')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"h_vi\"></a>\n",
    "#### [ VI ] - Nachfolgend werden noch weitere Funktionen hinzugefügt. Machen Sie sich mit der Funktionsweise vertraut.\n",
    "\n",
    "Weitere Funktionen wie Negation (`__neg__`), Subtraktion (`__sub__`) oder Division (`__truediv__`) können durch die zuvor definierten Funktionen beschrieben werden.\n",
    "Außerdem ist es sinnvoll die Funktionen auch bei vertauschter Reihenfolge der Operanden umzusetzen (Addition `__radd__`, Subtraktion `__rsub__`, Multiplikation `__rmul__`, Division `__rtruediv__`; `__r*__` steht dabei jeweils für \"right-hand operand\"), andernfalls würde eine Berechnung von `x = Value(5); y = 3 * x` einen Fehler verursachen, da keine Funktion existiert, die einen Integer-Wert mit einem Value multiplizieren kann.\n",
    "Die Funktion `__repr__` definiert, wie die Klasse als String ausgegeben wird.\n",
    "\n",
    "Diese Funktionen werden nachfolgend zu unserer Klasse hinzugefügt. Die Klasse `Value` enthält nun alle Funktionalitäten, um ein einfaches Neuronales Netzwerk mit linearer Ausgabe umsetzen zu können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Value(Value):\n",
    "    def __neg__(self):\n",
    "        \"\"\"negation: -self\"\"\"\n",
    "        return self * -1\n",
    "\n",
    "    def __sub__(self, other):\n",
    "        \"\"\"operator - (subtraction: self - other)\"\"\"\n",
    "        return self + (-other)\n",
    "\n",
    "    def __truediv__(self, other):\n",
    "        \"\"\"operator / (division: self / other)\"\"\"\n",
    "        return self * other ** -1\n",
    "\n",
    "    def __radd__(self, other):\n",
    "        \"\"\"operator + (addition with Value as right-hand operand: other + self)\"\"\"\n",
    "        return self + other\n",
    "\n",
    "    def __rsub__(self, other):\n",
    "        \"\"\"operator - (subtraction with Value as right-hand operand: other - self)\"\"\"\n",
    "        return other + (-self)\n",
    "\n",
    "    def __rmul__(self, other):\n",
    "        \"\"\"operator * (multiplication with Value as right-hand operand: other * self)\"\"\"\n",
    "        return self * other\n",
    "\n",
    "    def __rtruediv__(self, other):\n",
    "        \"\"\"operator / (division with Value as right-hand operand: other / self)\"\"\"\n",
    "        return other * self ** -1\n",
    "\n",
    "    # define how the Value class is printed\n",
    "    def __repr__(self):\n",
    "        \"\"\"string representation of this class\"\"\"\n",
    "        return f\"Value(data={self.data}, grad={self.grad})\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"h_vii\"></a>\n",
    "#### [ VII ] Wenden Sie die Klasse `Value` an, um für die nachfolgende Formel den Gradienten an der Stelle $x=0$ zu berechnen:\n",
    "\n",
    "$y = x - \\frac{1}{x^2 + 1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x =    # bitte Code ergaenzen <---------------- [Luecke (9)]\n",
    "y =    # bitte Code ergaenzen <---------------- [Luecke (10)]\n",
    "\n",
    "# reset gradient\n",
    "x.grad =    # bitte Code ergaenzen <---------------- [Luecke (11)]\n",
    "\n",
    "# calculate gradient\n",
    "y.    # bitte Code ergaenzen <---------------- [Luecke (12)]\n",
    "\n",
    "print(f\"y'(0) = {x.grad}\")\n",
    "\n",
    "# Implementierung ueberpruefen\n",
    "interactive_check('VII')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vollziehen Sie die Rechnung per Hand nach und überprüfen Sie, ob Sie auf dasselbe Ergebnis kommen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>&#9432; <i>Überprüfung &nbsp; &nbsp; <font color=\"CCCCCC\">(anklicken, um Lösung anzuzeigen)</font></i></summary>\n",
    "    <br>$\\frac{dy}{dx} = \\left[ \\frac{d}{dx} \\left( x - \\frac{1}{x^2 + 1} \\right) \\right] = \\left[ \\frac{d}{dx} x \\right] - \\left[ \\frac{d}{dx} \\frac{1}{x^2 + 1} \\right] = \\left[ 1 \\right] - \\left[ \\frac{\\left[ \\frac{d}{dx} 1 \\right] \\cdot \\left( x^2 + 1 \\right) - 1 \\cdot \\left[ \\frac{d}{dx} \\left( x^2 + 1 \\right) \\right]}{\\left( x^2 + 1 \\right)^2} \\right] = \\left[ 1 \\right] - \\left[ \\frac{\\left[ 0 \\right] \\cdot \\left( x^2 + 1 \\right) - 1 \\cdot \\left[ 2 \\cdot x \\right]}{\\left( x^2 + 1 \\right)^2} \\right] = 1 - \\frac{-2 \\cdot x}{\\left( x^2 + 1 \\right)^2}$\n",
    "    <br>$\\rightarrow 1 - \\frac{-2 \\cdot 0}{\\left( 0^2 + 1 \\right)^2} = 1 - \\frac{0}{1} = 1$\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-width: 5px\">\n",
    "\n",
    "<a name=\"regression_gerade\"></a>\n",
    "### (j) Realisieren Sie den betrachteten Lernschritt für die Regression der Geraden in dem implementierten minimalen Framework\n",
    "<a name=\"j_i\"></a>\n",
    "#### [ I ] - Nutzen Sie die Klasse `Value`, um die Regression einer Geraden für drei gegebene Punkte umzusetzen\n",
    "\n",
    "Umsetzung des Beispiels aus der Übungsaufgabe 2 (a-g) mittels automatischer Differentiation:\n",
    "Für drei gegebene Punkte im $\\mathbb{R}^2$ soll eine Gerade ermittelt werden, die den Funktionsverlauf der Punkte approximiert.\n",
    "Die Funktion der Gerade lässt sich durch ein einzelnes Neuron realisieren.\n",
    "Führen Sie mehrere Updateschritte für das Gewicht $w$ und den Bias $b$ des Neurons durch.\n",
    "Visualisieren Sie dabei Zwischenschritte und das Endergebnis analog zu den Zeichnungen in Aufgabe 2(a) und 2(g).\n",
    "\n",
    "Vorgegeben sind folgende drei Paare $(x_k, t_k), k \\in \\{ 1, 2, 3 \\}$ von Input und Teacher als Trainingsdaten:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1, t_1 = (1, 2)\n",
    "x_2, t_2 = (2, 2.3)\n",
    "x_3, t_3 = (3, 3)\n",
    "pts = [(x_1, t_1), (x_2, t_2), (x_3, t_3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Eingabe für das Neuron dient ein skalarer Wert $x \\in \\mathbb{R}$, der auf einen skalaren Wert $y \\in \\mathbb{R}$ abbilden soll (Regression): $y = w \\cdot x + b$. Folgende initiale Parameter des Neurons sind gegeben: $w(t=t_0) = 0$ und $b(t=t_0) = 0.5$. Verwenden Sie eine Lernrate von $\\eta = 0.09$ und den Mean Squared Error $E^{MSE} = \\frac{1}{N} \\sum_{k=1}^N \\left( y_k - t_k \\right)^2$ als Fehlermaß."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hilfsfunktionen für Visualisierung und Ausgabe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_line(pts, m, n, s=0):\n",
    "    x_coords = np.array(pts)[:, 0]\n",
    "    y_coords = np.array(pts)[:, 1]\n",
    "    x_values = np.linspace(0, 4, 200)\n",
    "    y_values = m * x_values + n\n",
    "    plt.plot(x_values, y_values, 'b-')\n",
    "    plt.plot(x_coords, y_coords, 'ro')\n",
    "    plt.axis([0, 4, 0, 4])\n",
    "    plt.show()\n",
    "\n",
    "def print_progress_line():\n",
    "    print(f'epoch {(epoch + 1)}:\\n\\tloss = {loss.data:.4f}, w = {w.data:.4f}, b = {b.data:.4f}\\n\\tdL/dw = {w.grad:.4f}, dL/db = {b.grad:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realisierung der Regression unter Verwendung der Klasse `Value`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize weight and bias weight (ensure to enable automatic differentiation by using the class Value)\n",
    "w =    # bitte Code ergaenzen <---------------- [Luecke (13)]\n",
    "b =    # bitte Code ergaenzen <---------------- [Luecke (14)]\n",
    "\n",
    "# set learning rate\n",
    "learning_rate = 0.09\n",
    "\n",
    "# show points and initial neuron configuration\n",
    "print('initial configuration:')\n",
    "visualize_line(pts, w.data, b.data)\n",
    "print('\\n')\n",
    "\n",
    "# fit neuron to points\n",
    "for epoch in range(500):   # train for 500 epochs\n",
    "    # reset gradients\n",
    "    w.grad =    # bitte Code ergaenzen <---------------- [Luecke (15)]\n",
    "    b.grad =    # bitte Code ergaenzen <---------------- [Luecke (16)]\n",
    "\n",
    "    # output of neuron (y_i = w * x_i + b) for all three inputs x_1, x_2, and x_3\n",
    "    y_1 =    # bitte Code ergaenzen <---------------- [Luecke (17)]\n",
    "    y_2 =    # bitte Code ergaenzen <---------------- [Luecke (18)]\n",
    "    y_3 =    # bitte Code ergaenzen <---------------- [Luecke (19)]\n",
    "\n",
    "    # calculate squared error for each sample: squared_error_i = (y_i - t_i) ** 2\n",
    "    squared_error_1 =    # bitte Code ergaenzen <---------------- [Luecke (20)]\n",
    "    squared_error_2 =    # bitte Code ergaenzen <---------------- [Luecke (21)]\n",
    "    squared_error_3 =    # bitte Code ergaenzen <---------------- [Luecke (22)]\n",
    "\n",
    "    # calculate loss (see formula above, use results for squared_error_i)\n",
    "    loss =    # bitte Code ergaenzen <---------------- [Luecke (23)]\n",
    "\n",
    "    # calculate gradients: call backward function for loss\n",
    "    loss.    # bitte Code ergaenzen <---------------- [Luecke (24)]\n",
    "\n",
    "    # update parameters with gradients: w_new = w_old - dL/dw * learning_rate\n",
    "    w.data =    # bitte Code ergaenzen <---------------- [Luecke (25)]\n",
    "    b.data =    # bitte Code ergaenzen <---------------- [Luecke (26)]\n",
    "\n",
    "    # report progress\n",
    "    if epoch in [0, 1, 9, 49, 99, 199, 299, 399]:\n",
    "        print_progress_line()\n",
    "        visualize_line(pts, w.data, b.data)\n",
    "        print('\\n')\n",
    "\n",
    "# report final result\n",
    "print('final result:')\n",
    "print_progress_line()\n",
    "visualize_line(pts, w.data, b.data)\n",
    "\n",
    "# Implementierung ueberpruefen\n",
    "interactive_check('j_I')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-width: 5px\">\n",
    "\n",
    "<a name=\"regression_parabel\"></a>\n",
    "### (m) Realisieren Sie das Neuronale Netzwerk und führen Sie so lange Lernschritte aus, bis die Parabel durch alle drei Punkte verläuft.\n",
    "\n",
    "Es kann keine Gerade gefunden werden, die den exakten Funktionsverlauf der drei gegebenen Punkte beschreibt.\n",
    "Das Regressionsproblem lässt sich dementsprechend mit einem einzelnen Neuron nicht fehlerfrei lösen.\n",
    "Daher soll nachfolgend eine Parabel zur Approximation des Funktionsverlaufs genutzt werden.\n",
    "Die Parameter einer Parabel lassen sich durch drei Punkte eindeutig beschreiben.\n",
    "Dementsprechend ist eine fehlerfreie Beschreibung des Funktionsverlauf durch eine Parabel möglich.\n",
    "Um eine Parabel durch eine Neuronales Netzwerk umzusetzen, muss ein zweischichtiges Neuronales Netzwerk verwendet werden\n",
    "mit je einem Neuron pro Schicht.\n",
    "Das erste Neuron muss eine quadratische Ausgabe haben, das Neuron der zweiten Schicht eine lineare Ausgabe.\n",
    "Das Neuronale Netzwerk lässt sich durch folgende Gleichung beschreiben:\n",
    "$$y = w^{(2)} \\cdot \\left(w^{(1)} \\cdot x + b^{(1)} \\right)^2 + b^{(2)}$$\n",
    "Dementsprechend können die drei Parameter der Parabel $(a, b, c)$ wie folgt bestimmt werden:\n",
    "\\begin{eqnarray}\n",
    "    a &=& w^{(1)2} \\cdot w^{(2)} \\\\\n",
    "    b &=& 2 \\cdot w^{(1)} \\cdot w^{(2)} \\cdot b^{(1)} \\\\\n",
    "    c &=& w^{(2)} \\cdot b^{(1)2} + b^{(2)}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"m_i\"></a>\n",
    "#### [ I ] - Nutzen Sie die Klasse `Value`, um die Regression einer Parabel in Form eines zweischichtigen Neuronalen Netzwerks für die drei gegebenen Punkte umzusetzen\n",
    "\n",
    "Umsetzung des Beispiels aus der Übungsaufgabe 2 (j) mittels automatischer Differentiation:\n",
    "Führen Sie mehrere Updateschritte für die Gewichte $w^{(1)}$, $b^{(1)}$, $w^{(2)}$ und $b^{(2)}$ des Neuronalen Netzwerks durch.\n",
    "Initialisieren Sie die Gewichte wie folgt:\n",
    "$$w^{(1)} = 0.01,\\quad b^{(1)} = 0.01,\\quad w^{(2)} = 0,\\quad b^{(2)} = 0.5$$\n",
    "Visualisieren Sie dabei Zwischenschritte und das Endergebnis analog zu den Zeichnungen in Aufgabe 2(a) und 2(g).\n",
    "Verwenden Sie eine Lernrate von $\\eta = 0.09$ und den Mean Squared Error $E^\\text{MSE}$ als Fehlermaß."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hilfsfunktionen für Visualisierung und Ausgabe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_parabola(pts, a, b, c):\n",
    "    x_coords = np.array(pts)[:, 0]\n",
    "    y_coords = np.array(pts)[:, 1]\n",
    "    x_values = np.linspace(0, 4, 200)\n",
    "    y_values = a * np.square(x_values) + b * x_values + c\n",
    "    plt.plot(x_values, y_values, 'b-')\n",
    "    plt.plot(x_coords, y_coords, 'ro')\n",
    "    plt.axis([0, 4, 0, 4])\n",
    "    plt.show()\n",
    "\n",
    "def print_progress_parabola(a, b, c):\n",
    "    print(f'epoch {(epoch + 1)}:\\n\\tloss = {loss.data:.4f}, w1 = {w1.data:.4f}, b1 = {b1.data:.4f}, w2 = {w2.data:.4f}, b2 = {b2.data:.4f}\\n\\tdL/dw1 = {w1.grad:.4f}, dL/db1 = {b1.grad:.4f}, dL/dw2 = {w2.grad:.4f}, dL/db2 = {b2.grad:.4f}\\n\\ta = {a:.4f}, b = {b:.4f}, c = {c:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realisierung der Regression unter Verwendung der Klasse `Value`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize weights and bias weights (ensure to enable automatic differentiation by using the class Value)\n",
    "w1 =    # bitte Code ergaenzen <---------------- [Luecke (27)]\n",
    "b1 =    # bitte Code ergaenzen <---------------- [Luecke (28)]\n",
    "w2 =    # bitte Code ergaenzen <---------------- [Luecke (29)]\n",
    "b2 =    # bitte Code ergaenzen <---------------- [Luecke (30)]\n",
    "\n",
    "# set learning rate\n",
    "learning_rate = 0.09\n",
    "\n",
    "# show points and parabola defined by initial neural network\n",
    "print('initial configuration:')\n",
    "# calculate parameters of parabola\n",
    "a = w1.data ** 2 * w2.data\n",
    "b = 2 * w1.data * w2.data * b1.data\n",
    "c = w2.data * b1.data ** 2 + b2.data\n",
    "visualize_parabola(pts, a, b, c)\n",
    "print('\\n')\n",
    "\n",
    "# fit neural network to points\n",
    "for epoch in range(4000):   # train for 4000 epochs\n",
    "    # reset gradients\n",
    "    w1.grad =    # bitte Code ergaenzen <---------------- [Luecke (31)]\n",
    "    b1.grad =    # bitte Code ergaenzen <---------------- [Luecke (32)]\n",
    "    w2.grad =    # bitte Code ergaenzen <---------------- [Luecke (33)]\n",
    "    b2.grad =    # bitte Code ergaenzen <---------------- [Luecke (34)]\n",
    "\n",
    "    # output of neuron (y_i = w2 * (w1 * x_i + b1) ** 2 + b2) for all three inputs x_1, x_2, and x_3\n",
    "    y_1 =    # bitte Code ergaenzen <---------------- [Luecke (35)]\n",
    "    y_2 =    # bitte Code ergaenzen <---------------- [Luecke (36)]\n",
    "    y_3 =    # bitte Code ergaenzen <---------------- [Luecke (37)]\n",
    "\n",
    "    # calculate squared error for each sample: squared_error_i = (y_i - t_i) ** 2\n",
    "    squared_error1 =    # bitte Code ergaenzen <---------------- [Luecke (38)]\n",
    "    squared_error2 =    # bitte Code ergaenzen <---------------- [Luecke (39)]\n",
    "    squared_error3 =    # bitte Code ergaenzen <---------------- [Luecke (40)]\n",
    "\n",
    "    # calculate loss (see formula above, use results for squared_error_i)\n",
    "    loss =    # bitte Code ergaenzen <---------------- [Luecke (41)]\n",
    "\n",
    "    # calculate gradients: call backward function for loss\n",
    "    loss.    # bitte Code ergaenzen <---------------- [Luecke (42)]\n",
    "\n",
    "    # update parameters with gradients: w_new = w_old - dL/dw * learning_rate\n",
    "    w1.data =    # bitte Code ergaenzen <---------------- [Luecke (43)]\n",
    "    b1.data =    # bitte Code ergaenzen <---------------- [Luecke (44)]\n",
    "    w2.data =    # bitte Code ergaenzen <---------------- [Luecke (45)]\n",
    "    b2.data =    # bitte Code ergaenzen <---------------- [Luecke (46)]\n",
    "\n",
    "    # report progress\n",
    "    if epoch in [0, 4, 49, 199, 299, 319, 349, 399, 999, 1999]:\n",
    "        a = w1.data ** 2 * w2.data\n",
    "        b = 2 * w1.data * w2.data * b1.data\n",
    "        c = w2.data * b1.data ** 2 + b2.data\n",
    "        print_progress_parabola(a, b, c)\n",
    "        visualize_parabola(pts, a, b, c)\n",
    "        print('\\n')\n",
    "\n",
    "# report final result\n",
    "print('final result:')\n",
    "a = w1.data ** 2 * w2.data\n",
    "b = 2 * w1.data * w2.data * b1.data\n",
    "c = w2.data * b1.data ** 2 + b2.data\n",
    "print_progress_parabola(a, b, c)\n",
    "visualize_parabola(pts, a, b, c)\n",
    "\n",
    "# Implementierung ueberpruefen\n",
    "interactive_check('m_I')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-width: 5px\">\n",
    "\n",
    "<a name=\"implementierung_neuronales_netzwerk\"></a>\n",
    "### (n) Erweitern Sie das Framework geeignet, sodass Sie ein Neuronales Netzwerk über abstrakte Schnittstellen realisieren können, das für handgeschriebene Ziffern unterscheiden kann, ob es sich um eine gerade oder ungerade Zahl handelt.\n",
    "\n",
    "Nachfolgend soll gezeigt werden, wie basierend auf automatischer Differentiation durch Autograd ein einfaches Framework für die Verwendung Neuronaler Netzwerke erstellt werden kann.\n",
    "\n",
    "---\n",
    "<a name=\"n_i\"></a>\n",
    "#### [ I ] - Machen Sie sich mit der Klasse `Module` vertraut.\n",
    "\n",
    "Die Basis für die Umsetzung des einfachen Frameworks für die Verwendung Neuronaler Netzwerke stellt die Klasse `Module` dar, die folgende Methoden bereitstellt:\n",
    "* `zero_grad` setzt die Gradienten aller Parameter auf Null.\n",
    "* `parameters` gibt eine Liste aller Parameter zurück, für die partielle Ableitungen berechnet werden können. Diese Methode muss in abgeleiteten Klassen überschrieben werden, um die entsprechenden Parameter zurückzugeben.\n",
    "* `forward` stellt eine Methode dar, die beim Forward Pass aufgerufen wird. Diese Methode muss in abgeleiteten Klassen überschrieben werden, um die entsprechende Berechnung beim Forward Pass umzusetzen.\n",
    "* `__call__` ruft die Methode `forward` auf und setzt eine einfachere Verwendung der abgeleiteten Klassen um (statt `module = Module(); module.forward(x)` kann mittels `module(x)` der Forward Pass aufgerufen werden)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Module:\n",
    "    \"\"\"base class\"\"\"\n",
    "\n",
    "    def zero_grad(self):\n",
    "        \"\"\"reset gradient for all parameters\"\"\"\n",
    "        for p in self.parameters():    # iterate through the list of all parameters\n",
    "            p.grad = 0    # parameter p is of type Value, it's gradient is set to zero\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"return the parameters, should be overwritten in derived classes\"\"\"\n",
    "        return []\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"forward pass for input x, should be overwritten in derived classes\"\"\"\n",
    "        return None\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"call function for forward pass with input x\"\"\"\n",
    "        return self.forward(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"n_ii\"></a>\n",
    "#### [ II ] - Implementieren Sie die Klasse `Neuron`.\n",
    "\n",
    "Ein Neuron in einem Multi Layer Perceptron (MLP) setzt die Funktion $z = \\sum\\limits_{i=1}^m \\left( w_i \\cdot x_i \\right) + b$ für einen $m$-dimensionalen Input $\\mathbf{\\underline{x}}$ um (Skalarprodukt-Aktivierung).\n",
    "Dementsprechend besitzt es $m$ Gewichte $w_i, i = \\{ 1, \\ldots, m \\}$ und ein Bias-Gewicht $b$.\n",
    "Implementieren Sie eine Klasse `Neuron`, die von der Klasse `Module` abgeleitet ist und diese Funktion im Forward Pass realisiert.\n",
    "Initialisieren Sie die Gewichte $w_i$ zufällig gleichverteilt im Bereich $\\left[ -\\sqrt{\\frac{1}{fan_{in}}} , \\sqrt{\\frac{1}{fan_{in}}} \\right]$, wobei $fan_{in}$ der Dimensionalität des Inputs entspricht und das Bias-Gewicht mit einem Wert von Null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "class Neuron(Module):\n",
    "    \"\"\"implements an abstract interface for a single neuron\"\"\"\n",
    "\n",
    "    def __init__(self, in_features):\n",
    "        \"\"\"creation and initialization of this neuron for in_features input neurons\"\"\"\n",
    "        minimum_value = -(1 / in_features) ** 0.5\n",
    "        maximum_value = (1 / in_features) ** 0.5\n",
    "        self.w = []    # the neuron needs one weight for each input neuron\n",
    "        for _ in range(in_features):    # nin = number of input neurons\n",
    "            initial_value_w_i = random.uniform(minimum_value, maximum_value)    # random uniform initialization\n",
    "            w_i = Value(initial_value_w_i)    # make this weight differentiable, set initial value\n",
    "            self.w.append(w_i)\n",
    "        # add one differentiable bias weight, initialize it with 0\n",
    "        self.b =    # bitte Code ergaenzen <---------------- [Luecke (47)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"return the neuron's activation for input x\"\"\"\n",
    "        activation = 0    # initial activation is zero\n",
    "        for w_i, x_i in zip(self.w, x):    # iterate through all inputs and weights\n",
    "            # multiply weight w_i with input x_i and add it to the activation\n",
    "            activation +=    # bitte Code ergaenzen <---------------- [Luecke (48)]\n",
    "        activation += self.b    # add bias\n",
    "        return activation\n",
    "\n",
    "    def parameters(self):    # overwrite function of base class\n",
    "        \"\"\"return the neuron's weights and bias\"\"\"\n",
    "        # concatenate the list of weights with the list of the single bias weight\n",
    "        parameter_list = self.w + [self.b]\n",
    "        return parameter_list\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"string representation of this class\"\"\"\n",
    "        return f'Neuron({len(self.w)} inputs)'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um zu testen, ob die gewünschte Funktionalität umgesetzt wurde, können Sie nachfolgenden Code ausführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neuron = Neuron(2)\n",
    "neuron.w[0].data = 1\n",
    "neuron.w[1].data = 2\n",
    "neuron.zero_grad()\n",
    "activation = neuron([3, 4])\n",
    "activation.backward()\n",
    "print(f'neuron = {neuron}')\n",
    "print(f'parameters of the neuron = {neuron.parameters()}')\n",
    "print(f'activation for input [3, 4] = {activation}')\n",
    "\n",
    "# Implementierung ueberpruefen\n",
    "interactive_check('n_II')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"n_iii\"></a>\n",
    "#### [ III ] - Implementieren Sie eine Klasse, die eine vollverschaltete Schicht umsetzt.\n",
    "\n",
    "In einer vollverschalteten Schicht verarbeitet jedes Neuron den kompletten Output der vorherigen Schicht als Input.\n",
    "Implementieren Sie eine Klasse, die von der Klasse `Module` abgeleitet ist und eine durch den Parameter `out_features` festgelegte Anzahl an Neuronen besitzt.\n",
    "Die Klasse soll [wie in PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html#torch.nn.Linear) als Linear bezeichnet werden.\n",
    "Im Forward Pass muss die Aktivierung für jedes Neuron berechnet werden.\n",
    "Die Liste der Aktivierungen aller Neuronen stellt die Ausgabe dieser Schicht dar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \"\"\"implements an abstract interface for a fully connected layer\"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        \"\"\"creation and initialization of this layer for in_features input neurons\n",
    "        and out_features neurons in this layer\"\"\"\n",
    "        self.neurons = []    # start with empty list\n",
    "        for _ in range(out_features):    # add nout neurons\n",
    "            neuron = Neuron(in_features)    # create and initialize a new neuron\n",
    "            self.neurons.append(neuron)    # add it to the list\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"return the activations of all neurons in this layer for input x\"\"\"\n",
    "        activations = []    # store results in list, start with empty list\n",
    "        for neuron in self.neurons:    # iterate through all neurons in this layer\n",
    "            # calculate the activation of this neuron\n",
    "            activation =    # bitte Code ergaenzen <---------------- [Luecke (49)]\n",
    "            # add it to the list\n",
    "            activations.append(activation)\n",
    "        if len(activations) == 1:    # if there is only a single neuron\n",
    "            return activations[0]    # return its activation\n",
    "        else:    # otherwise\n",
    "            return activations    # return the list of activations for all neurons in this layer\n",
    "\n",
    "    def parameters(self):    # overwrite function of base class\n",
    "        \"\"\"return weights and bias of all neurons in this layer\"\"\"\n",
    "        parameters = []    # store all parameters in a list, start with empty list\n",
    "        for neuron in self.neurons:    # iterate through all neurons in this layer\n",
    "            neuron_i_parameters = neuron.parameters()    # get list of parameters for this neuron\n",
    "            parameters = parameters + neuron_i_parameters    # concatenate the lists\n",
    "        return parameters\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"string representation of this class\"\"\"\n",
    "        return f'FCLayer[{\", \".join(str(neuron) for neuron in self.neurons)}]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um zu testen, ob die gewünschte Funktionalität umgesetzt wurde, können Sie nachfolgenden Code ausführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fc1 = Linear(2, 2)\n",
    "fc2 = Linear(2, 1)\n",
    "fc1.neurons[0].w[0].data = 1\n",
    "fc1.neurons[0].w[1].data = 2\n",
    "fc1.neurons[1].w[0].data = 3\n",
    "fc1.neurons[1].w[1].data = 4\n",
    "fc2.neurons[0].w[0].data = 5\n",
    "fc2.neurons[0].w[1].data = 6\n",
    "fc1.zero_grad()\n",
    "fc2.zero_grad()\n",
    "activation = fc2(fc1([7, 8]))\n",
    "activation.backward()\n",
    "print(f'fc1 = {fc1}')\n",
    "print(f'fc2 = {fc2}')\n",
    "print(f'parameters of fc1 = {fc1.parameters()}')\n",
    "print(f'parameters of fc2 = {fc2.parameters()}')\n",
    "print(f'activation for input [7, 8] = {activation}')\n",
    "\n",
    "# Implementierung ueberpruefen\n",
    "interactive_check('n_III')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"n_iv\"></a>\n",
    "#### [ IV ] - Implementieren Sie die Ausgabefunktionen ReLU und Sigmoid.\n",
    "\n",
    "Damit ein mehrschichtiges Neuronales Netzwerk sinnvoll eingesetzt werden kann, muss nach jeder Aktivierung noch eine nichtlineare Ausgabe erfolgen. Für Hiddenschichten ist eine ReLU-Ausgabe sinnvoll, für eine Ausgabe, die auf zwei Klassen (Ausgabe 0 oder 1) abbilden soll, ist eine Sigmoid-Ausgabe sinnvoll.\n",
    "\n",
    "Die nichtlineare Ausgabefunktion muss einen einzelnen Wert verarbeiten können, aber auch eine Liste von Werten, die das Ergebnis der Ausgabe einer Schicht ist. Daher ist zunächst eine Fallunterscheidung notwendig, ob die Eingabe `x` eine Liste ist (`isinstance(x, (list, tuple))`) oder ob es sich um einen einzelnen Wert handelt. Falls es sich um eine Liste handelt, wird die nichtlineare Ausgabefunktion für jeden Wert der Liste einzeln aufgerufen und die Liste der Ergebnisse wird zurückgegeben.\n",
    "\n",
    "Das Ergebnis der Berechnung im Forward Pass wird wieder in einem `Value` gespeichert (Variable `out`). Als Kind-Knoten wird die Eingabe `x` übergeben. Außerdem muss noch eine Funktion für die Berechnung des Backward Pass (`_backward`) bereitgestellt werden. Im Backward Pass wird zunächst die Ableitung der jeweiligen nichtlinearen Ausgabefunktion berechnet und anschließend mit dem Gradienten am Ausgang der Funktion multipliziert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    if isinstance(x, (list, tuple)):    # in case of more then one input\n",
    "        out = []    # output will be a list of multiple elements, start with empty list\n",
    "        for x_i in x:    # for each element x_i in the list x\n",
    "            out_i = relu(x_i)    # calculate the element's non-linear output\n",
    "            out.append(out_i)    # and add it to the list\n",
    "        return out    # return the list of outputs\n",
    "    else:\n",
    "        # x needs to be an instance of class Value\n",
    "        if not isinstance(x, Value):\n",
    "            x = Value(x)\n",
    "\n",
    "        # calculate the relu output\n",
    "        forward_result = max(0, x.data)\n",
    "\n",
    "        # create a new Value that can store the result and the calculation tree\n",
    "        out = Value(forward_result, (x,))\n",
    "\n",
    "        # backward path: deviation\n",
    "        def _backward():\n",
    "            # calculate derivative of activation function (relu)\n",
    "            if forward_result > 0:\n",
    "                derivative_relu = 1\n",
    "            else:\n",
    "                derivative_relu = 0\n",
    "            # update gradient\n",
    "            x.grad += derivative_relu * out.grad\n",
    "\n",
    "        # add this backward function to the new node\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out\n",
    "\n",
    "def sigmoid(x):\n",
    "    if isinstance(x, (list, tuple)):    # in case of more then one input\n",
    "        out = []    # output will be a list of multiple elements, start with empty list\n",
    "        for x_i in x:    # for each element x_i in the list x\n",
    "            out_i = sigmoid(x_i)    # calculate the element's non-linear output\n",
    "            out.append(out_i)    # and add it to the list\n",
    "        return out    # return the list of outputs\n",
    "    else:\n",
    "        # x needs to be an instance of class Value\n",
    "        if not isinstance(x, Value):\n",
    "            x = Value(x)\n",
    "\n",
    "        # calculate the sigmoid output\n",
    "        forward_result = 1 / (1 + np.exp(-x.data))\n",
    "\n",
    "        # create a new Value that can store the result and the calculation tree\n",
    "        out =    # bitte Code ergaenzen <---------------- [Luecke (50)]\n",
    "\n",
    "        # backward path: deviation\n",
    "        def _backward():\n",
    "            # calculate derivative of activation function (sigmoid)\n",
    "            derivative_sigmoid = forward_result * (1 - forward_result)\n",
    "            # update gradient\n",
    "            x.grad +=    # bitte Code ergaenzen <---------------- [Luecke (51)]\n",
    "\n",
    "        # add this backward function to the new node\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um zu testen, ob die gewünschte Funktionalität umgesetzt wurde, können Sie nachfolgenden Code ausführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = [Value(-7), Value(-1), Value(0), Value(1), Value(7)]\n",
    "x2 = [Value(-7), Value(-1), Value(0), Value(1), Value(7)]\n",
    "y1 = relu(x1)\n",
    "y2 = sigmoid(x2)\n",
    "for i in range(len(x1)):\n",
    "    x1[i].grad = 0\n",
    "    x2[i].grad = 0\n",
    "    y1[i].backward()\n",
    "    y2[i].backward()\n",
    "\n",
    "print(f'x1 = {x1}')\n",
    "print(f'x2 = {x2}')\n",
    "print(f'y1 = {y1}')\n",
    "print(f'y2 = {y2}')\n",
    "\n",
    "# Implementierung ueberpruefen\n",
    "interactive_check('n_IV')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"n_v\"></a>\n",
    "#### [ V ] Laden Sie den Digit-Datensatz und führen Sie die nachfolgend spezifizierte Datenvorverarbeitung durch.\n",
    "\n",
    "Der erste Schritt in der Deep-Learning-Verarbeitungskette ist die Datensatzerzeugung.\n",
    "Der [Digit-Datensatz](https://scikit-learn.org/stable/auto_examples/datasets/plot_digits_last_image.html#sphx-glr-auto-examples-datasets-plot-digits-last-image-py) stellt bereits handgeschriebene Ziffern mit zugehörigen Labeln zur Verfügung.\n",
    "Er ist über die Python-Bibliothek [scikit-learn](https://scikit-learn.org) durch die Funktion [`sklearn.datasets.load_digits`](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html) verfügbar.\n",
    "Laden Sie den Datensatz und visualisieren Sie die ersten zehn handgeschriebenen Ziffern des Datensatzes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load the digits dataset\n",
    "digits_dataset = datasets.load_digits()\n",
    "\n",
    "def visualize_image(subplot, image, label):\n",
    "    subplot.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "    subplot.set_title(f'label: {label}')\n",
    "\n",
    "# display the first 10 images with labels\n",
    "fig, axs = plt.subplots(1, 10, figsize=(16, 1.2))    # 10 subplots\n",
    "for index in range(10):\n",
    "    visualize_image(axs[index], digits_dataset.images[index], digits_dataset.target[index])\n",
    "plt.show()    # show plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>&#9432; <i>Überprüfung &nbsp; &nbsp; <font color=\"CCCCCC\">(anklicken, um Lösung anzuzeigen)</font></i></summary>\n",
    "    <br>Folgende Grafik sollte angezeigt werden:\n",
    "    <br><img src=\"data:image/png;base64, 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\" alt=\"samples of digit dataset\" />\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der zweite Schritt in der Deep-Learning-Verarbeitungskette ist die Datenvorverarbeitung.\n",
    "Wandeln Sie die Bilder in einen eindimensionalen Vektor um, dessen Werte im Intervall \\[0, 1\\] liegen.\n",
    "Hinweis: Die Werte der Pixel in Bildern des Digit-Datensatzes liegen im Intervall \\[0, 16\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert image to floating point matrix\n",
    "x = digits_dataset.images.astype('float32')\n",
    "\n",
    "# reshape input x such that it is a 1D vector for each sample\n",
    "samples, rows, cols = x.shape\n",
    "x = x.reshape(samples,    # bitte Code ergaenzen <---------------- [Luecke (52)]\n",
    "\n",
    "# normalize values to range [0, 1]\n",
    "x =    # bitte Code ergaenzen <---------------- [Luecke (53)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Label des Digit-Datensatzes geben an, um welche Ziffer es sich handelt.\n",
    "Verarbeiten Sie die Label derart, dass sie für die Unterscheidung von geraden und ungeraden Zahlen geeignet sind.\n",
    "Gerade Zahlen sollen dabei das Klassenlabel 0 bekommen und ungerade Zahlen das Klassenlabel 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get labels as teacher\n",
    "t = digits_dataset.target\n",
    "\n",
    "# change the labels to 0 for even numbers and 1 for odd numbers\n",
    "for i in range(len(t)):\n",
    "    if t[i] % 2 == 0:    # even number\n",
    "        t[i] =    # bitte Code ergaenzen <---------------- [Luecke (54)]\n",
    "    else:    # odd number\n",
    "        t[i] =    # bitte Code ergaenzen <---------------- [Luecke (55)]\n",
    "\n",
    "# convert teacher to array\n",
    "t = np.array(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun müssen die Daten noch in Trainings- und Validierungsdaten aufgeteilt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data to training and validation data (use 15% of data for validation)\n",
    "x_train, x_val, t_train, t_val = train_test_split(x, t, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die korrekte Funktionsweise eines Neuronalen Netzwerks nachzuweisen, sollte zunächst nur ein einzelner Batch verwendet werden.\n",
    "Das Neuronale Netzwerk muss in der Lage sein, diesen Trainingsbatch auswendig zu lernen und einen Fehler nahe Null zu erzeugen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use only 32 samples as one single batch for training to show that the implemented framework works\n",
    "x_train = x_train[:32, :]\n",
    "t_train = t_train[:32]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um zu testen, ob die gewünschte Funktionalität umgesetzt wurde, können Sie nachfolgenden Code ausführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print(f'(number of samples, feature dimension) of x_train = {x_train.shape}')\n",
    "print(f'(number of samples, feature dimension) of x_val = {x_val.shape}')\n",
    "print(f'number of samples of t_train = {t_train.shape}')\n",
    "print(f'number of samples of t_val = {t_val.shape}')\n",
    "print(f'unique labels in t_train = {np.unique(t_train)}')\n",
    "print(f'minimum value of x_train = {np.min(x_train)}')\n",
    "print(f'maximum value of x_train = {np.max(x_train)}')\n",
    "\n",
    "# Implementierung ueberpruefen\n",
    "interactive_check('n_V')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"n_vi\"></a>\n",
    "#### [ VI ] Erstellen Sie unter Nutzung der bisher erstellten Klassen ein zweischichtiges MLP mit ReLU-Ausgabe in der Hiddenschicht und Sigmoid-Ausgabe in der Ausgabeschicht, das eine binäre Klassifikation der Bilder des Digit-Datensatzes realisiert.\n",
    "\n",
    "Der dritte Schritt in der Deep-Learning-Verarbeitungskette ist die Modellauswahl.\n",
    "Nutzen Sie die zuvor erstellten Klassen, um ein zweischichtiges Multi-Layer Perceptron (MLP) zu erstellen, das wie folgt aufgebaut ist:\n",
    "* 64-dimensionale Eingabe\n",
    "* erste Schicht: vollverschaltet, 10 Hiddenneuronen, ReLU-Ausgabe\n",
    "* zweite Schicht: vollverschaltet, 1 Ausgabeneuron, Sigmoid-Ausgabe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the neural network\n",
    "class Network(Module):\n",
    "    \"\"\"implements a multi layer perceptron (MLP)\n",
    "    with one hidden layer of 10 neurons and one output layer of a single neuron\n",
    "\n",
    "    hint: construction similar to PyTorch\n",
    "          --> compare with \"Code Beispiel 2.9 [2/4] CNN in PyTorch: Netzwerk definieren\" in slides of chapter 2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"create and initialize the two fully connected layers\"\"\"\n",
    "        # create fully connected layer: 64-dimensional input, 10 hidden neurons\n",
    "        self.fc1 = Linear(64, 10)\n",
    "        # create fully connected layer: 10 hidden neurons as input, 1 output neuron\n",
    "        self.fc2 =    # bitte Code ergaenzen <---------------- [Luecke (56)]\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"forward pass for input x\"\"\"\n",
    "        # calculation of 1st layer: input x --> fully connected layer --> relu\n",
    "        x = relu(self.fc1(x))\n",
    "        # calculation of 2nd layer: output of 1st layer x --> fully connected layer --> sigmoid\n",
    "        x =    # bitte Code ergaenzen <---------------- [Luecke (57)]\n",
    "        return x\n",
    "\n",
    "    def parameters(self):\n",
    "        \"\"\"return parameters (weights, bias) of all layers in the network\"\"\"\n",
    "        fc1_parameters = self.fc1.parameters()    # get the list of parameters for 1st layer\n",
    "        fc2_parameters = self.fc2.parameters()    # get the list of parameters for 2nd layer\n",
    "        return fc1_parameters + fc2_parameters    # concatenate the lists\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"call function for forward pass with input x\n",
    "        type checking needed to enable batch processing in case x is a numpy array\"\"\"\n",
    "        if isinstance(x, (np.ndarray, np.float32)):    # in case it is a numpy array\n",
    "            if x.ndim > 1:    # if it has more than one dimension (batch dimension + feature dimension)\n",
    "                x = x.tolist()    # make it a list\n",
    "                y = []    # output will be a list of multiple elements, start with empty list\n",
    "                for x_i in x:    # for each element x_i in the list x\n",
    "                    y_i = self.forward(x_i)    # calculate the output for sample x_i\n",
    "                    y.append(y_i)    # and add it to the list\n",
    "                return y    # return the list of outputs\n",
    "            else:    # only the feature dimension\n",
    "                x = x.tolist()    # make it a list\n",
    "                self.forward(x)    # process it\n",
    "        else:    # in case x is a single input\n",
    "            return self.forward(x)    # process it\n",
    "\n",
    "    def __repr__(self):\n",
    "        \"\"\"string representation of this class\"\"\"\n",
    "        return f'MLP[{str(self.fc1)} --> ReLU --> {str(self.fc2)} --> Sigmoid]'\n",
    "\n",
    "# set random seed for reproducibility (weight initialization is random)\n",
    "random.seed(1337)\n",
    "\n",
    "# create the neural network\n",
    "net =    # bitte Code ergaenzen <---------------- [Luecke (58)]\n",
    "\n",
    "# print information about the neural network\n",
    "print(net)\n",
    "print(f'number of parameters = {len(net.parameters())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>&#9432; <i>Überprüfung &nbsp; &nbsp; <font color=\"CCCCCC\">(anklicken, um Lösung anzuzeigen)</font></i></summary>\n",
    "    <br>Es sollte folgendes angezeigt werden:\n",
    "    <br><code>MLP[FCLayer[Neuron(64 inputs), Neuron(64 inputs), Neuron(64 inputs), Neuron(64 inputs), Neuron(64 inputs), Neuron(64 inputs), Neuron(64 inputs), Neuron(64 inputs), Neuron(64 inputs), Neuron(64 inputs)] --> ReLU --> FCLayer[Neuron(10 inputs)] --> Sigmoid]\n",
    "number of parameters = 661</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"n_vii\"></a>\n",
    "#### [ VII ] Implementieren Sie die benötigten Funktionen zur Realisierung des Trainings.\n",
    "\n",
    "Um die Parameter des Neuronalen Netzwerks im Training anpassen zu können, muss ein Fehler berechnet werden, der zu den einzelnen Parametern zurückpropagiert werden kann.\n",
    "Verwenden Sie den Binary Cross Entropy (BCE) Loss für $N$ Trainingsbeispiele:\n",
    "$$E^{BCE} = \\frac{1}{N} \\cdot \\sum_{i=1}^{N} t_i \\cdot ln(y_i) + (1 - t_i) \\cdot ln(1 - y_i)$$\n",
    "\n",
    "Für dessen Berechnung muss zunächst noch die Logarithmus-Funktion so umgesetzt werden, dass sie differenzierbar ist:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def log(x):\n",
    "    if isinstance(x, (list, tuple)):    # in case of more then one input\n",
    "        out = []    # output will be a list of multiple elements, start with empty list\n",
    "        for x_i in x:    # for each element x_i in the list x\n",
    "            out_i = log(x_i)    # calculate the element's non-linear output\n",
    "            out.append(out_i)    # and add it to the list\n",
    "        return out    # return the list of outputs\n",
    "    else:\n",
    "        # x needs to be an instance of class Value\n",
    "        if not isinstance(x, Value):\n",
    "            x = Value(x)\n",
    "\n",
    "        # calculate the logarithm output\n",
    "        forward_result = np.log(x.data)\n",
    "\n",
    "        # create a new Value (self.__class__) that can store the result and the calculation tree\n",
    "        out = Value(forward_result, (x,))\n",
    "\n",
    "        # backward path: deviation\n",
    "        def _backward():\n",
    "            # calculate derivative of activation function (log)\n",
    "            derivative_log = 1.0 / x.data\n",
    "            # update gradient\n",
    "            x.grad += derivative_log * out.grad\n",
    "\n",
    "        # add this backward function to the new node\n",
    "        out._backward = _backward\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementieren Sie den Binary Cross Entropy (BCE) Loss, der die Prädiktionen des Neuronalen Netzwerks $\\mathbf{\\underline{y}}$ und die zugehörigen Teacher-Werte $\\mathbf{\\underline{t}}$ als Eingabe bekommt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy_loss(y, t):\n",
    "    # initialize loss with zero\n",
    "    loss = 0\n",
    "\n",
    "    # loop throug all outputs y and labels t simultaniously\n",
    "    for y_i, t_i in zip(y, t):\n",
    "        # calculate loss for this output and teacher (see above for the equation inside the sum)\n",
    "        loss_i =    # bitte Code ergaenzen <---------------- [Luecke (59)]\n",
    "        # add it to overall loss\n",
    "        loss += loss_i\n",
    "\n",
    "    # devide loss by the number of samples\n",
    "    n_samples = len(y)\n",
    "    loss = -(1 / n_samples) * loss\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um zu testen, ob die gewünschte Funktionalität umgesetzt wurde, können Sie nachfolgenden Code ausführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [Value(0.9), Value(0.1), Value(0.4)]\n",
    "t = [1, 0, 1]\n",
    "loss = binary_cross_entropy_loss(y, t)\n",
    "for i in range(len(y)):\n",
    "    y[i].grad = 0\n",
    "loss.backward()\n",
    "\n",
    "# Implementierung ueberpruefen\n",
    "interactive_check('n_VII')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als zusätzliches Bewertungsmaß soll die Accuracy berechnet werden.\n",
    "Dafür werden die Ausgaben des Neuronalen Netzwerkes $\\mathbf{\\underline{y}}$ binarisiert und mit den Klassenlaben $\\mathbf{\\underline{t}}$ verglichen.\n",
    "Die Accuracy gibt den Anteil der übereinstimmenden Klassenlabel im Verhältnis zu allen Labeln an.\n",
    "Implementieren Sie die Funktion zur Berechnung der Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, t):\n",
    "    # count number of samples and number of correct predictions\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    # loop throug all outputs y and labels t simultaniously\n",
    "    for y_i, t_i in zip(y, t):\n",
    "        # binarize the decisions of the neural network (class label 1 if y_i > 0.5, class label 0 otherwise)\n",
    "        if y_i.data > 0.5:\n",
    "            predicted_class =    # bitte Code ergaenzen <---------------- [Luecke (60)]\n",
    "        else:\n",
    "            predicted_class =    # bitte Code ergaenzen <---------------- [Luecke (61)]\n",
    "\n",
    "        # count this sample\n",
    "        total += 1\n",
    "\n",
    "        # increase number of correct predictions if prediction for current sample is correct\n",
    "        if predicted_class == t_i:\n",
    "            correct += 1\n",
    "\n",
    "    # calculate accuracy\n",
    "    acc = correct / total\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um zu testen, ob die gewünschte Funktionalität umgesetzt wurde, können Sie die nachfolgenden Ausgaben mit der Musterlösung vergleichen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [Value(0.9), Value(0.1), Value(0.4)]\n",
    "t = [1, 0, 1]\n",
    "acc = accuracy(y, t)\n",
    "print(f'acc = {acc}')\n",
    "\n",
    "# Implementierung ueberpruefen\n",
    "interactive_check('n_VII_acc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Während eines Trainingsschritts wird zunächst ein Forward Pass ausgeführt.\n",
    "Dann müssen die Gradienten des vorherigen Trainingsschritts zurückgesetzt werden.\n",
    "Anschließend kann der Backward Pass durchgeführt werden, um die Gradienten zu ermitteln, mit denen alle Parameter des Neuronalen Netzwerks adaptiert werden können.\n",
    "Implementieren Sie die Funktion `train_step`, die die beschriebene Vorgehensweise umsetzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(X, t, net):\n",
    "    # forward pass with input data X\n",
    "    y =    # bitte Code ergaenzen <---------------- [Luecke (62)]\n",
    "\n",
    "    # compute loss and accuracy\n",
    "    loss =    # bitte Code ergaenzen <---------------- [Luecke (63)]\n",
    "    training_data_accuracy =    # bitte Code ergaenzen <---------------- [Luecke (64)]\n",
    "\n",
    "    # reset gradients from previous train step\n",
    "    net.    # bitte Code ergaenzen <---------------- [Luecke (65)]\n",
    "\n",
    "    # calculate all gradients\n",
    "    loss.    # bitte Code ergaenzen <---------------- [Luecke (66)]\n",
    "\n",
    "    # update every parameter with sgd\n",
    "    learning_rate = 0.25\n",
    "    for parameter in net.parameters():\n",
    "        parameter.data = parameter.data - parameter.grad * learning_rate\n",
    "\n",
    "    return net, loss, training_data_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"n_viii\"></a>\n",
    "#### [ VIII ] Implementieren Sie eine Funktion zur Realisierung des Tests auf Validierungsdaten.\n",
    "\n",
    "Für die Valdierung ist nur der Forward Pass notwendig. Anschließend werden die Ausgaben des Neuronalen Netzwerks mit den Labeln verglichen, um die Bewertungsmaße zu berechnen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(X, t, net):\n",
    "    # forward pass with input data X\n",
    "    y =    # bitte Code ergaenzen <---------------- [Luecke (67)]\n",
    "\n",
    "    # calculate performance measures (compare network outputs y and teacher t)\n",
    "    loss =    # bitte Code ergaenzen <---------------- [Luecke (68)]\n",
    "    acc =    # bitte Code ergaenzen <---------------- [Luecke (69)]\n",
    "\n",
    "    return loss, acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"n_ix\"></a>\n",
    "#### [ IX ] Trainieren und validieren Sie das Neuronale Netzwerk.\n",
    "\n",
    "Nachfolgend soll das Neuronale Netzwerk trainiert werden.\n",
    "Führen Sie 100 Trainingsschritte aus.\n",
    "Validieren Sie nach jedem fünften Trainingsschritt.\n",
    "Speichern Sie für jeden Trainings- und Validierungsschritt den Loss und die Accuracy.\n",
    "\n",
    "<p>&#9432; <i>Hinweis:</i> Beobachten Sie die Ausgaben während des Trainings.\n",
    "    Legen Sie Ihre Aufmerksamkeit zunächst auf den Loss bezüglich der Trainingsdaten.\n",
    "    Am Anfang sind nur Fortschritte bei diesem Bewertungsmaß zu beobachten, diese wirken sich aber noch nicht auf die Accuracy aus.\n",
    "    Erst nach etwa dem zehnten Trainingsschritt zeigen sich auch deutliche Verbesserungen bei der Accuracy.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lists to store performance measures during training\n",
    "loss_list = []\n",
    "accuracy_list = []\n",
    "val_loss_list = []\n",
    "val_accuracy_list = []\n",
    "\n",
    "# start training\n",
    "for step in range(100):    # 100 training steps\n",
    "    # perform one training step (training data: x_train, t_train; neural network: net)\n",
    "    net, loss, train_accuracy =    # bitte Code ergaenzen <---------------- [Luecke (70)]\n",
    "\n",
    "    # store performance measures\n",
    "    loss_list.append(loss.data)\n",
    "    accuracy_list.append(train_accuracy * 100)\n",
    "\n",
    "    # print output and validate every 5th training step\n",
    "    if step % 5 == 0:\n",
    "        # perform one validation step (validation data: x_val, t_val; neural network: net)\n",
    "        val_loss, val_accuracy =    # bitte Code ergaenzen <---------------- [Luecke (71)]\n",
    "\n",
    "        # store performance measures\n",
    "        val_loss_list.append(val_loss.data)\n",
    "        val_accuracy_list.append(val_accuracy * 100)\n",
    "\n",
    "        # print training progress\n",
    "        print(f\"step {step:>3}:  training:   loss = {loss.data:.4f}, accuracy = {(train_accuracy * 100):.3f}%\\n           validation: loss = {val_loss.data:.4f}, accuracy = {(val_accuracy * 100):.3f}%\\n\")\n",
    "\n",
    "# final validation step\n",
    "val_loss, val_accuracy = test_step(x_val, t_val, net)\n",
    "\n",
    "# store final performance measures\n",
    "loss_list.append(loss.data)\n",
    "accuracy_list.append(train_accuracy * 100)\n",
    "val_loss_list.append(val_loss.data)\n",
    "val_accuracy_list.append(val_accuracy * 100)\n",
    "\n",
    "# print final training result\n",
    "print('final result:')\n",
    "print(f\"step 100:  training:   loss = {loss.data:.4f}, accuracy = {(train_accuracy * 100):.3f}%\\n           validation: loss = {val_loss.data:.4f}, accuracy = {(val_accuracy * 100):.3f}%\\n\")\n",
    "\n",
    "# Implementierung ueberpruefen\n",
    "interactive_check('n_IX')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"n_x\"></a>\n",
    "#### [ X ] Visualisieren Sie die Bewertungsmaße Loss und Accuracy über den Verlauf des Trainings. Was ist zu beobachten?\n",
    "\n",
    "Visualisieren Sie die während des Trainings gespeicherten Bewertungsmaße Loss und Accuracy für die Trainings- und Validierungsdaten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot loss curves (data stored in loss_list and val_loss_list)\n",
    "plt.plot(np.arange(len(loss_list)), loss_list,\n",
    "         'b-', label='training')    # blue color, solid line for training curve\n",
    "plt.plot(np.arange(len(val_loss_list)) * 5, val_loss_list,\n",
    "         'b:', label='validation')    # blue color, dotted line for validation curve\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('training steps')\n",
    "plt.legend(loc='lower left')\n",
    "plt.axis([0, 100, 0, 0.71])\n",
    "plt.show()\n",
    "\n",
    "# plot accuracy curves (data stored in accuracy_list and val_accuracy_list)\n",
    "plt.plot(    # bitte Code ergaenzen <---------------- [Luecke (72)]\n",
    "         'r-', label='training')    # red color, solid line for training curve\n",
    "plt.plot(    # bitte Code ergaenzen <---------------- [Luecke (73)]\n",
    "         'r:', label='validation')    # red color, dotted line for validation curve\n",
    "plt.ylabel('accuracy [%]')\n",
    "plt.xlabel('training steps')\n",
    "plt.legend(loc='lower right')\n",
    "plt.axis([0, 100, 49.5, 100.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>&#9432; <i>Überprüfung &nbsp; &nbsp; <font color=\"CCCCCC\">(anklicken, um Lösung anzuzeigen)</font></i></summary>\n",
    "    <br>Folgende Grafiken sollten angezeigt werden:\n",
    "    <br><img src=\"data:image/png;base64, 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\" alt=\"loss curve\" />\n",
    "    <br><img src=\"data:image/png;base64, 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\" alt=\"accuracy curve\" />\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was ist während des Trainingsverlaufs zu beobachten? Woran können Sie erkennen, ob die Implementierung korrekt ist?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>&#9432; <i>Überprüfung &nbsp; &nbsp; <font color=\"CCCCCC\">(anklicken, um Lösung anzuzeigen)</font></i></summary>\n",
    "    <p>Bis etwa zum 40. Trainingsschritt reduziert sich der Loss sowohl auf Trainings- als auch Validierungsdaten.\n",
    "    Das heißt, das Neuronale Netzwerk lernt eine Klassifikation, die auch gut auf unbekannte Daten generalisiert.\n",
    "    Nach dem 40. Trainingsschritt fällt der Loss auf Trainingsdaten weiter, auf Validierungsdaten steigt er jedoch wieder an.\n",
    "    Das Neuronale Netzwerk lernt in dieser Phase die Trainingsdaten auswendig und kann dadurch schlechter auf unbekannten Daten generalisieren.\n",
    "    Nach etwa dem 85. Trainingsschritt ist das Neuronale Netzwerk in der Lage alle Trainingsdaten perfekt zu klassifizieren (Accuracy erreicht 100%).\n",
    "    Der Anteil der korrekten Klassifikationen auf den unbekannten Validierungsdaten (Accuracy) bleibt jedoch ab etwa der 15. Epoche auf einem konstanten Niveau von etwa 81-84%.</p>\n",
    "    <p>Da das Neuronale Netzwerk in der Lage ist, den einzelnen Batch der 32 Trainingsdaten auswendig zu lernen, ist davon auszugehen, dass die Implementierung korrekt ist. Um eine bessere Generalisierung zu erreichen sind mehr Trainingsdaten notwendig. Außerdem sollten geeignete Trainings- und Regularisierungstechniken eingesetzt werden, die in Kapitel 3 \"Grundlagen\" behandelt werden. Auch der Einsatz tiefer und moderner Netzwerkarchitekturen, die in Kapitel 4 \"Architekturen\" behandelt werden, kann durch deren Kerntechnologien die Leistungsfähigkeit verbessern.</p>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$_{_\\text{Created for Deep Learning for Computer Vision (DL4CV)}}$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "a36bc1df758689cab85bd3cddbf19760f70742c7b9e9867a3c5834cb2b9245b9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}