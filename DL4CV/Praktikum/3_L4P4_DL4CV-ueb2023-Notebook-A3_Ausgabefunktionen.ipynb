{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aufgabe 3 - Ausgabefunktionen\n",
    "\n",
    "Dieses Notebook thematisiert die Implementierung und Verwendung von Ausgabefunktionen.\n",
    "\n",
    "<font color=\"#aa0000\">**Hinweis:**</font>\n",
    "Dieses Notebook enthält Praktikumsaufgaben ([P3](#praktikum)). Erweitern Sie das Notebook geeignet und speichern Sie das ausgeführte Notebook erneut ab (File &rarr; Download as &rarr; Notebook). Reichen Sie abschließend das heruntergeladene Notebook im zugehörigen [Moodle-Kurs](https://moodle.tu-ilmenau.de/course/view.php?id=28) ein.\n",
    "\n",
    "**Die Einreichungsfrist finden Sie im Moodle-Kurs.**\n",
    "\n",
    "### Inhaltsverzeichnis\n",
    "- (b) [Implementierung von Ausgabefunktionen mit NumPy](#b)\n",
    "- [Praktikumsaufgabe P3 zu GeLU und Hard-Swish](#praktikum)\n",
    "- (c) [Implementierung von Ausgabefunktionen mit PyTorch](#c)\n",
    "- (e) [Reproduktion der Ergebnisse aus Teilaufgabe (d) mit NumPy und PyTorch](#e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-width: 5px\">\n",
    "\n",
    "### Vorbereitung\n",
    "Wichtige Ergebnisse können während der Bearbeitung überprüft werden. Grundvoraussetzung hierfür ist, dass Sie das Paket `tui-dl4cv` <font color=\"#aa0000\">installieren bzw. aktualisieren</font> und anschließend importieren.\n",
    "\n",
    "Für die Installation stehen Ihnen zwei mögliche Wege zur Verfügung.\n",
    "\n",
    "**(1) Installation direkt in diesem Notebook:**\n",
    "Führen Sie den nachfolgenden Code-Block aus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "print(f\"Automatically install package for '{sys.executable}'\")\n",
    "!{sys.executable} -m pip install tui-dl4cv \\\n",
    "    --extra-index-url \"https://2023ws:QSv2EKuu9MmyPAZzez82@nikrgl.informatik.tu-ilmenau.de/api/v4/projects/1730/packages/pypi/simple\" \\\n",
    "    --no-cache --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ODER\n",
    "\n",
    "**(2) Manuelle Installation über die Konsole:**\n",
    "Öffnen Sie eine Konsole (\"Anaconda Prompt\" unter Windows) und führen Sie folgenden Befehl aus:\n",
    "```text\n",
    "pip install tui-dl4cv --extra-index-url \"https://2023ws:QSv2EKuu9MmyPAZzez82@nikrgl.informatik.tu-ilmenau.de/api/v4/projects/1730/packages/pypi/simple\" --no-cache --upgrade\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Führen Sie abschließend folgenden Code-Block aus, um das Paket verwenden zu können.**\n",
    "Während der Bearbeitung können Sie nun Ihre Ergebnisse mithilfe der Funktion `interactive_check` überprüfen. Die Funktionsaufrufe sind bereits an den entsprechenden Stellen im Notebook enthalten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tui_dl4cv.activation\n",
    "\n",
    "# noetige Erweiterung, damit Variablen aus diesem Notebook automatisch ueberprueft werden koennen\n",
    "def interactive_check(name, **kwargs):\n",
    "    tui_dl4cv.activation.interactive_check(name, globals(), **kwargs)\n",
    "\n",
    "# fuer Visualisierung der Ausgabefunktionen und deren Ableitungen\n",
    "from tui_dl4cv.activation import plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-width: 5px\">\n",
    "\n",
    "<a name=\"b\"></a>\n",
    "### (b) Implementieren Sie die besprochenen Ausgabefunktionen und deren Ableitungen in Python mithilfe von NumPy.\n",
    "Beachten Sie bei der Implementierung, dass die Funktionen in vektorisierter Form umgesetzt werden sollen. Jede Funktion soll also stets die Ausgaben für mehrere Aktivierungen berechnen.\n",
    "\n",
    "<br>\n",
    "<div style=\"background-color: #FAEAEA; padding: 5px; margin: 5px 0px 5px 0px; border-radius: 5px;\">\n",
    "Folgende NumPy-Funktionen könnten für die Vervollständigung der Lücken in diesem Jupyter Notebook hilfreich sein:\n",
    "    <ul style=\"margin-bottom: 0px\">\n",
    "        <li><code style=\"background-color: #FAEAEA;\">np.exp</code>&nbsp;&nbsp;&rarr;&nbsp;<a href=\"https://numpy.org/doc/stable/reference/generated/numpy.exp.html\" target=\"_blank\">NumPy-Dokumentation</a>\n",
    "        </li>\n",
    "        <li><code style=\"background-color: #FAEAEA;\">np.log</code>&nbsp;&nbsp;&rarr;&nbsp;<a href=\"https://numpy.org/doc/stable/reference/generated/numpy.log.html\" target=\"_blank\">NumPy-Dokumentation</a>\n",
    "        </li>\n",
    "        <li><code style=\"background-color: #FAEAEA;\">np.tanh</code>&nbsp;&nbsp;&rarr;&nbsp;<a href=\"https://numpy.org/doc/stable/reference/generated/numpy.tanh.html\" target=\"_blank\">NumPy-Dokumentation</a>\n",
    "        </li>\n",
    "        <li><code style=\"background-color: #FAEAEA;\">np.abs</code>&nbsp;&nbsp;&rarr;&nbsp;<a href=\"https://numpy.org/doc/stable/reference/generated/numpy.absolute.html\" target=\"_blank\">NumPy-Dokumentation</a>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Pakete importieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# erlaubt das partielle Definieren von Funktionsargumenten\n",
    "from functools import partial\n",
    "\n",
    "# numerische Berechnungen\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Die angestrebte Vektorisierung von Berechnungen beschleunigt die Bestimmung der Funktionsausgaben für mehrere Elemente in der Eingabe.\n",
    "Das nachfolgende Beispiel für die Berechnung der Gradienten für Leaky ReLU soll Ihnen verdeutlichen, wie ein (in Python) zeitaufwendiges Iterieren über alle Elemente durch Vektorisierung vermieden werden kann:\n",
    "\n",
    "*Elementweise Berechnungsvorschrift:*\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial y_i}{\\partial z_i} = \\begin{cases} \\alpha & z_i \\le 0 \\\\ 1 & z_i > 0\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "*Implementierung:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slow_leaky_relu_backward(z, alpha=0.01):\n",
    "    assert z.ndim == 1, \"Nur fuer 1D-Eingaben konzipiert\"\n",
    "    dydz = np.empty_like(z)\n",
    "    for i, z_i in enumerate(z):\n",
    "        # Ausgabe fuer jedes Element z_i bestimmen\n",
    "        if z_i <= 0:\n",
    "            dydz[i] = alpha\n",
    "        else:\n",
    "            dydz[i] = 1\n",
    "    return dydz\n",
    "\n",
    "def fast_leaky_relu_backward(z, alpha=0.01):\n",
    "    # betrachtete Faelle schliessen sich aus -> Realisierung durch Addition moeglich\n",
    "    dydz = (z <= 0) * alpha + (z > 0) * 1\n",
    "    return dydz\n",
    "\n",
    "def even_faster_leaky_relu_backward(z, alpha=0.01):\n",
    "    # Default-Ausgabewert: 1\n",
    "    dydz = np.ones_like(z)\n",
    "    # Teile in Ausgabe korrigieren\n",
    "    dydz[z <= 0] = alpha\n",
    "    return dydz\n",
    "\n",
    "\n",
    "# Funktionsausgaben visualisieren -> Werteverlaufgleichheit pruefen\n",
    "plot(forward_functions=(),\n",
    "     backward_functions=(slow_leaky_relu_backward,\n",
    "                         fast_leaky_relu_backward,\n",
    "                         even_faster_leaky_relu_backward),\n",
    "     labels=('slow', 'fast', 'even faster'),\n",
    "     linestyles=('-', '--', ':'))\n",
    "\n",
    "# Ausfuehrung timen und vergleichen\n",
    "z = np.linspace(-8, 8, 1000)\n",
    "%timeit -n 100 slow_leaky_relu_backward(z)\n",
    "%timeit -n 100 fast_leaky_relu_backward(z)\n",
    "%timeit -n 100 even_faster_leaky_relu_backward(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Fazit:*\n",
    "Der Plot zeigt, dass alle drei Implementierungen das gleiche Ergebnis realisieren, wobei die letzten beiden Varianten die Berechnung deutlich beschleunigen. Die letzte Variante reduziert die Anzahl der vektorisierten Vergleiche auf Eins und sorgt daher noch einmal für eine schnellere Berechnung.\n",
    "\n",
    "\n",
    "---\n",
    "#### Sigmoid\n",
    "\\begin{equation}\n",
    "    y = f_{\\text{Sigmoid}}(z) = \\frac{1}{1+ e^{-z}},\\quad\\quad\\frac{dy}{dz} = f_{\\text{Sigmoid}}(z) \\cdot (1-f_{\\text{sigmoid}}(z))\n",
    "\\end{equation}\n",
    "\n",
    "*Implementierung:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_forward(z):\n",
    "    return     # bitte Code ergaenzen <---------------- [Luecke (1)]\n",
    "\n",
    "def sigmoid_backward(z):\n",
    "    return     # bitte Code ergaenzen <---------------- [Luecke (2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Visualisierung & Überprüfung:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Funktionsverlauf visualisieren\n",
    "plot(forward_functions=sigmoid_forward,\n",
    "     backward_functions=sigmoid_backward,\n",
    "     labels='Sigmoid')\n",
    "\n",
    "# Implementierung ueberpruefen\n",
    "interactive_check('sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 5px solid #004B83;\">\n",
    "<img alt=\"\" src=\"data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0idXRmLTgiPz4KPHN2ZyB2ZXJzaW9uPSIxLjEiIGlkPSJMYXllcl8xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB4PSIwcHgiIHk9IjBweCIKCSB4bWw6c3BhY2U9InByZXNlcnZlIiB3aWR0aD0iNDEwIiBoZWlnaHQ9IjExMCI+CiAgPHBvbHlnb24gcG9pbnRzPSIwLDAgIDEwMCwxMDAgNDAwLDEwMCA0MDAsIDAiIGZpbGw9Im5vbmUiIHN0cm9rZT0iIzAwNEI4MyIgc3Ryb2tlLXdpZHRoPSI0IiAvPgogIDx0ZXh0IHg9IjEzMCIgeT0iNjUiIHN0eWxlPSJmb250OiBib2xkIDUwcHggc2Fucy1zZXJpZjsgZmlsbDogIzAwNEI4MzsiPmZha3VsdGF0aXY8L3RleHQ+Cjwvc3ZnPgoK\" style=\"position: absolute; right: 10px; margin-top: -20px; width: 12%;\"/>\n",
    "\n",
    "#### Tangens hyperbolicus\n",
    "\\begin{equation}\n",
    "    y = f_{\\text{Tanh}}(z) = \\tanh(z),\\quad\\quad\\frac{dy}{dz} = 1 - f_{\\text{Tanh}}(z)^2\n",
    "\\end{equation}\n",
    "*Implementierung:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh_forward(z):\n",
    "    return     # bitte Code ergaenzen <---------------- [Luecke fakultativ (1)]\n",
    "\n",
    "def tanh_backward(z):\n",
    "    return     # bitte Code ergaenzen <---------------- [Luecke fakultativ (2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Visualisierung & Überprüfung:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Funktionsverlauf visualisieren\n",
    "plot(forward_functions=tanh_forward,\n",
    "     backward_functions=tanh_backward,\n",
    "     labels='Tanh')\n",
    "\n",
    "# Implementierung ueberpruefen\n",
    "interactive_check('tanh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Sigmoid und Tanh im Vergleich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Funktionsverlauf visualisieren\n",
    "plot(forward_functions=(sigmoid_forward, tanh_forward),\n",
    "     backward_functions=(sigmoid_backward, tanh_backward),\n",
    "     labels=('Sigmoid', 'Tanh'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Verständnisfragen:*\n",
    "\n",
    "Mit welchen Problemen können Sie bei der Verwendung dieser Funktionen in den Hidden-Schichten eines Neurobalen Netzwerks konfrontiert werden? Welche Funktion ist besser für die Verwendung in Hidden-Schichten geeignet?\n",
    "<br>\n",
    "<br>\n",
    "<details>\n",
    "    <summary>&#9432; <i>Überprüfung &nbsp; &nbsp; <font color=\"CCCCCC\">(anklicken, um Lösung anzuzeigen)</font></i></summary>\n",
    "    <ul>\n",
    "        <li>Die Probleme werden bei der Vorstellung beider Funktionen im Rahmen der Vorlesung thematisiert &#x1F609;</li>\n",
    "        <li>Tanh sollte Sigmoid in Hidden-Schichten vorgezogen werden</li>\n",
    "    </ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Softplus\n",
    "\\begin{equation}\n",
    "    y = f_{\\text{Softplus}}(z) = \\ln(1+e^z),\\quad\\quad\\frac{dy}{dz} = f_{\\text{Sigmoid}}(z)\n",
    "\\end{equation}\n",
    "*Implementierung:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softplus_forward(z):\n",
    "    return     # bitte Code ergaenzen <---------------- [Luecke fakultativ (3)]\n",
    "\n",
    "def softplus_backward(z):\n",
    "    return     # bitte Code ergaenzen <---------------- [Luecke fakultativ (4)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Visualisierung & Überprüfung:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Funktionsverlauf visualisieren\n",
    "plot(forward_functions=softplus_forward,\n",
    "     backward_functions=softplus_backward,\n",
    "     labels='Softplus')\n",
    "\n",
    "# Implementierung ueberpruefen\n",
    "interactive_check('softplus')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 5px solid #004B83;\">\n",
    "\n",
    "#### Rectified Linear Unit (ReLU), Leaky ReLU, ReLU-n\n",
    "\\begin{equation}\n",
    "    y = f_{\\text{ReLU}}(z, \\alpha, n) = \\min(\\max(\\alpha \\cdot z, z), n),\\quad\\quad\\frac{\\partial y}{\\partial z} = \\begin{cases} \\alpha & z \\le 0\\\\ 1 & 0 < z \\le n \\\\ 0 & z > n\\end{cases}\n",
    "\\end{equation}\n",
    "\n",
    "Beachten Sie, dass `max` und `min` in der Implementierung ebenfalls auf eine Addition bzw. Subtraktion zurückgeführt werden kann.\n",
    "\n",
    "*Implementierung:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_forward(z, alpha=0, n=None):\n",
    "    # relu & leaky relu\n",
    "    # bitte Code ergaenzen <---------------- [Luecke (3)]\n",
    "\n",
    "    # Erweiterung auf relu-n\n",
    "    if n is not None:\n",
    "        # bitte Code ergaenzen <---------------- [Luecke (4)]\n",
    "        pass\n",
    "\n",
    "    return # bitte Code ergaenzen <---------------- [Luecke (5)]\n",
    "\n",
    "def relu_backward(z, alpha=0, n=None):\n",
    "    # relu & leaky relu\n",
    "    # bitte Code ergaenzen <---------------- [Luecke (6)]\n",
    "\n",
    "    # Erweiterung auf relu-n\n",
    "    if n is not None:\n",
    "        # bitte Code ergaenzen <---------------- [Luecke (7)]\n",
    "        pass\n",
    "\n",
    "    return     # bitte Code ergaenzen <---------------- [Luecke (8)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Visualisierung & Überprüfung:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Funktionsverlauf aller drei Varianten visualisieren\n",
    "plot(forward_functions=(relu_forward,\n",
    "                        partial(relu_forward, alpha=0.01),\n",
    "                        partial(relu_forward, n=6)),\n",
    "     backward_functions=(relu_backward,\n",
    "                         partial(relu_backward, alpha=0.01),\n",
    "                         partial(relu_backward, n=6)),\n",
    "     labels=('ReLU', 'Leaky ReLU (alpha: 0.01)', 'ReLU-n (n: 6)'),\n",
    "     linestyles=('-', '--', ':'))\n",
    "\n",
    "# Implementierung ueberpruefen\n",
    "interactive_check('relu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Verständnisfragen:*\n",
    "\n",
    "Was wird als Dying-ReLU-Problem bezeichnet? Welche ReLU-Variante eignet sich, um dem Dying-ReLU-Problem zu begegnen?\n",
    "\n",
    "Was versteht man unter dem Exploding-Activation-Problem? Welche ReLU-Variante kann zur Reduktion dieses Problems verwendet werden?\n",
    "<br>\n",
    "<br>\n",
    "<details>\n",
    "    <summary>&#9432; <i>Überprüfung &nbsp; &nbsp; <font color=\"CCCCCC\">(anklicken, um Lösung anzuzeigen)</font></i></summary>\n",
    "    <ul>\n",
    "        <li>Beide Probleme werden bei der Vorstellung der ReLU-Varianten im Rahmen der Vorlesung thematisiert &#x1F609;</li>\n",
    "        <li>Leaky-ReLU beugt dem Dying-ReLU-Problem vor</li>\n",
    "        <li>ReLU-n wirkt dem Exploding-Activation-Problem entgegen</li>\n",
    "    </ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 5px solid #004B83;\">\n",
    "<img alt=\"\" src=\"data:image/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0idXRmLTgiPz4KPHN2ZyB2ZXJzaW9uPSIxLjEiIGlkPSJMYXllcl8xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB4PSIwcHgiIHk9IjBweCIKCSB4bWw6c3BhY2U9InByZXNlcnZlIiB3aWR0aD0iNDEwIiBoZWlnaHQ9IjExMCI+CiAgPHBvbHlnb24gcG9pbnRzPSIwLDAgIDEwMCwxMDAgNDAwLDEwMCA0MDAsIDAiIGZpbGw9Im5vbmUiIHN0cm9rZT0iIzAwNEI4MyIgc3Ryb2tlLXdpZHRoPSI0IiAvPgogIDx0ZXh0IHg9IjEzMCIgeT0iNjUiIHN0eWxlPSJmb250OiBib2xkIDUwcHggc2Fucy1zZXJpZjsgZmlsbDogIzAwNEI4MzsiPmZha3VsdGF0aXY8L3RleHQ+Cjwvc3ZnPgoK\" style=\"position: absolute; right: 10px; margin-top: -20px; width: 12%;\"/>\n",
    "\n",
    "\n",
    "#### Exponential Linear Unit (ELU)\n",
    "\\begin{equation}\n",
    "    y = f_{\\text{ELU}}(z, \\alpha) = \\begin{cases} \\alpha \\cdot (e^z-1) & z \\le 0 \\\\ z & z > 0\\end{cases},\\quad\\quad\\frac{\\partial y}{\\partial z} = \\begin{cases} f_{\\text{ELU}}(z, \\alpha) + \\alpha & z \\le 0\\\\ 1 & z > 0\\end{cases}\n",
    "\\end{equation}\n",
    "*Implementierung:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu_forward(z, alpha=1.0):\n",
    "    return     # bitte Code ergaenzen <---------------- [Luecke fakultativ (5)]\n",
    "\n",
    "\n",
    "def elu_backward(z, alpha=1.0):\n",
    "    return     # bitte Code ergaenzen <---------------- [Luecke fakultativ (6)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Visualisierung & Überprüfung:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktionsverlauf visualisieren\n",
    "plot(forward_functions=(elu_forward,\n",
    "                        partial(elu_forward, alpha=0.5)),\n",
    "     backward_functions=(elu_backward,\n",
    "                         partial(elu_backward, alpha=0.5)),\n",
    "     labels=('ELU', 'ELU (alpha: 0.5)'))\n",
    "\n",
    "# Implementierung ueberpruefen\n",
    "interactive_check('elu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-top: 5px solid #004B83;\">\n",
    "\n",
    "#### Swish\n",
    "\\begin{equation}\n",
    "    y = f_{\\text{Swish}}(z, \\beta) = z \\cdot f_{\\text{sigmoid}}(\\beta \\cdot z),\\quad\\quad\\frac{\\partial y}{\\partial z} = \\beta \\cdot f_{\\text{Swish}}(z, \\beta) + f_{\\text{Sigmoid}}(\\beta \\cdot z)(1 - \\beta \\cdot f_{\\text{Swish}}(z, \\beta))\n",
    "\\end{equation}\n",
    "*Implementierung:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swish_forward(z, beta=1.0):\n",
    "    return     # bitte Code ergaenzen <---------------- [Luecke (9)]\n",
    "\n",
    "def swish_backward(z, beta=1.0):\n",
    "    return     # bitte Code ergaenzen <---------------- [Luecke (10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Visualisierung & Überprüfung:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktionsverlauf visualisieren\n",
    "plot(forward_functions=(swish_forward,\n",
    "                        partial(swish_forward, beta=0.1),\n",
    "                        partial(swish_forward, beta=10.0)),\n",
    "     backward_functions=(swish_backward,\n",
    "                         partial(swish_backward, beta=0.1),\n",
    "                         partial(swish_backward, beta=10.0)),\n",
    "     labels=('Swish', 'Swish (beta: 0.1)', 'Swish (beta: 10.0)'))\n",
    "\n",
    "# Implementierung ueberpruefen\n",
    "interactive_check('swish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-width: 5px\">\n",
    "\n",
    "<a name=\"praktikum\"></a>\n",
    "<h3 style=\"color: #aa0000;\">Praktikumsaufgabe P3: GELU und Hard-Swish</h3>\n",
    "\n",
    "Die Ausgabefunktion *Swish* ist verwandt mit der weniger bekannten Ausgabefunktion *GELU*. *GELU* steht für *Gaussian Error Linear Units* und ist wie folgt definiert:\n",
    "\n",
    "\\begin{equation}\n",
    "    y = f_{GELU}(z) \\; = \\; z \\cdot \\Phi(z) \\; = \\; z \\cdot \\frac{1}{2} \\left[ 1 + \\text{erf} \\left( \\frac{z}{\\sqrt{2}} \\right) \\right]\n",
    "\\end{equation}\n",
    "\n",
    "wobei $\\Phi(z)$ für die kummulative Verteilungsfunktion der Normalverteilung steht, welche häufig durch die [Fehlerfunktion](https://de.wikipedia.org/wiki/Fehlerfunktion) berechnet wird.\n",
    "\n",
    "Da die Berechnung dieser Fehlerfunktion sehr rechenaufwendig ist, kann GELU unter anderem auch mit Swish-beta approximiert werden mit:\n",
    "\n",
    "\\begin{equation}\n",
    "     y = f_{\\text{Swish}}\\left(z, 1.702\\right)\n",
    "\\end{equation}\n",
    "\n",
    "Implementieren Sie zunächst die exakte Berechnung von GELU und leiten Sie die Ableitung nach $z$ her, wenn gilt:\n",
    "\n",
    "\\begin{equation}\n",
    "    \\frac{d}{dx} \\left(  \\text{erf}(x) \\right) = \\frac{2}{\\sqrt{\\pi}} \\cdot e^{-x^2}\n",
    "\\end{equation}\n",
    "\n",
    "Vergleichen Sie anschließend den Verlauf sowie die Berechnungszeit mit der approximierten Variante.\n",
    "\n",
    "<br>\n",
    "<div style=\"background-color: #FAEAEA; padding: 5px; margin: 5px 0px 5px 0px; border-radius: 5px;\">\n",
    "Folgende Funktionen und Konstanten könnten für die Vervollständigung der Lücken in diesem Jupyter Notebook hilfreich sein:\n",
    "    <ul style=\"margin-bottom: 0px\">\n",
    "        <li><code style=\"background-color: #FAEAEA;\">scipy.special.erf</code>&nbsp;&nbsp;&rarr;&nbsp;<a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.special.erf.html\" target=\"_blank\">SciPy-Dokumentation</a>\n",
    "        </li>\n",
    "        <li><code style=\"background-color: #FAEAEA;\">np.sqrt</code>&nbsp;&nbsp;&rarr;&nbsp;<a href=\"https://numpy.org/doc/stable/reference/generated/numpy.sqrt.html\" target=\"_blank\">NumPy-Dokumentation</a>\n",
    "        </li>\n",
    "        <li><code style=\"background-color: #FAEAEA;\">np.exp</code>&nbsp;&nbsp;&rarr;&nbsp;<a href=\"https://numpy.org/doc/stable/reference/generated/numpy.exp.html\" target=\"_blank\">NumPy-Dokumentation</a>\n",
    "        </li>\n",
    "        <li><code style=\"background-color: #FAEAEA;\">np.pi</code>&nbsp;&nbsp;&rarr;&nbsp;<a href=\"https://numpy.org/doc/stable/reference/constants.html#numpy.pi\" target=\"_blank\">NumPy-Dokumentation</a>\n",
    "        </li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "<br>\n",
    "<details>\n",
    "    <summary>&#9432; <i>Hinweise zur Ableitung &nbsp; &nbsp; <font color=\"CCCCCC\">(anklicken, um Hilfestellung anzuzeigen)</font></i></summary>\n",
    "    <br>\n",
    "    Achten Sie auf die Verwendung von Produkt- und Kettenregel! (siehe Übungsunterlagen)\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.special\n",
    "\n",
    "def gelu_forward(z):\n",
    "    return     # bitte Code ergaenzen <---------------- [Luecke (11)]\n",
    "\n",
    "def gelu_backward(z):\n",
    "    return     # bitte Code ergaenzen <---------------- [Luecke (12)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktionsverlauf visualisieren\n",
    "plot(forward_functions=(gelu_forward,\n",
    "                        partial(swish_forward, beta=1.702)\n",
    "                       ),\n",
    "     backward_functions=(gelu_backward,\n",
    "                         partial(swish_backward, beta=1.702)\n",
    "                        ),\n",
    "     labels=('GELU', 'Swish (beta: 1.702)'), debug=True)\n",
    "\n",
    "# Implementierung ueberpruefen\n",
    "interactive_check('gelu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausfuehrung timen und vergleichen\n",
    "z = np.linspace(-8, 8, 10000)\n",
    "print('Forward Pass:')\n",
    "print('gelu: \\t', end='')\n",
    "%timeit -n 500 gelu_forward(z)\n",
    "print('swish: \\t', end='')\n",
    "%timeit -n 500 swish_forward(z, beta=1.702)\n",
    "print('\\nBackward Pass:')\n",
    "print('gelu: \\t', end='')\n",
    "%timeit -n 500 gelu_backward(z)\n",
    "print('swish: \\t', end='')\n",
    "%timeit -n 500 swish_backward(z, beta=1.702)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Obwohl *Swish* bereits deutlich effizienter als *GELU* ist, wird in einigen Neuronalen Netzwerken, die speziell für mobile Anwendungen konzipiert sind, eine weitere Approximation in Form von *Hard-Swish* verwendet:\n",
    "\n",
    "\\begin{equation}\n",
    "    y = f_{hSwish}(z) = z \\cdot \\frac{f_{ReLU}(z=z+3, \\alpha=0, n=6)}{6}\n",
    "\\end{equation}\n",
    "\n",
    "Realisieren Sie die Berechnung der Ausgabefunktion in vektorisierter Form.\n",
    "\n",
    "Überlegen Sie weiterhin, wie die partielle Ableitung nach $z$ aussehen könnte und implementieren Sie diese ebenfalls in vektorisierter Form.\n",
    "\n",
    "Verwenden Sie in beiden Implementierungen bereits definierte Funktionen wieder.\n",
    "\n",
    "Vergleichen Sie anschließend den Verlauf sowie die Berechnungszeit von *Swish* und *Hard-Swish*.\n",
    "\n",
    "*Implementierung:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hswish_forward(z):\n",
    "    return     # bitte Code ergaenzen <---------------- [Luecke (13)]\n",
    "\n",
    "def hswish_backward(z):\n",
    "    return     # bitte Code ergaenzen <---------------- [Luecke (14)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Visualisierung & Überprüfung:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktionsverlauf visualisieren\n",
    "plot(forward_functions=(swish_forward,\n",
    "                        hswish_forward),\n",
    "     backward_functions=(swish_backward,\n",
    "                         hswish_backward),\n",
    "     labels=('Swish', 'Hard-Swish'),\n",
    "     linestyles=('-', '--'))\n",
    "\n",
    "# Implementierung ueberpruefen\n",
    "interactive_check('hswish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausfuehrung timen und vergleichen\n",
    "z = np.linspace(-8, 8, 10000)\n",
    "print('Forward Pass:')\n",
    "print('swish: \\t\\t', end='')\n",
    "%timeit -n 500 swish_forward(z)\n",
    "print('hswish: \\t', end='')\n",
    "%timeit -n 500 hswish_forward(z)\n",
    "print('\\nBackward Pass:')\n",
    "print('swish: \\t\\t', end='')\n",
    "%timeit -n 500 swish_backward(z)\n",
    "print('hswish: \\t', end='')\n",
    "%timeit -n 500 hswish_backward(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-width: 5px\">\n",
    "\n",
    "<a name=\"c\"></a>\n",
    "### (c) Erweitern Sie die Implementierung anschließend so, dass die Ausgaben ebenfalls mit PyTorch berechnet werden können."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Zusätzliche Pakete importieren:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### PyTorch-Implementierungen\n",
    "Implementieren Sie alle Forward-Funktionen ebenfalls mit PyTorch. Achten Sie darauf, dass alle Parameter korrekt auf die PyTorch-Bezeichner übertragen werden. Backward-Funktionen müssen dank AutoGrad nicht händisch implementiert werden.\n",
    "\n",
    "<br>\n",
    "<div style=\"background-color: #FAEAEA; padding: 5px; margin: 5px 0px 5px 0px; border-radius: 5px;\">\n",
    "Zusätzlich zu den Vorlesungsunterlagen stellt die Dokumentation von PyTorch den wichtigsten Anlaufpunkt zur Vervollständigung der Lücken dar: <a href=\"https://pytorch.org/docs/stable/nn.functional.html\" target=\"_blank\">PyTorch-NN-Functional-Dokumentation</a>\n",
    "</div>\n",
    "\n",
    "*Implementierungen:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_pytorch(z):\n",
    "    return     # bitte Code ergaenzen <---------------- [Luecke (15)]\n",
    "\n",
    "def tanh_pytorch(z):\n",
    "    return     # bitte Code ergaenzen <---------------- [Luecke fakultativ (7)]\n",
    "\n",
    "def softplus_pytorch(z):\n",
    "    return # bitte Code ergaenzen <---------------- [Luecke fakultativ (8)]\n",
    "\n",
    "def relu_pytorch(z, alpha=0.0, n=None):\n",
    "    if n is None:\n",
    "        return     # bitte Code ergaenzen <---------------- [Luecke (16)]\n",
    "    return     # bitte Code ergaenzen <---------------- [Luecke (17)]\n",
    "\n",
    "def elu_pytorch(z, alpha=1.0):\n",
    "    return     # bitte Code ergaenzen <---------------- [Luecke fakultativ (9)]\n",
    "\n",
    "def swish_pytorch(z, beta=1.0):\n",
    "    return     # bitte Code ergaenzen <---------------- [Luecke (18)]\n",
    "\n",
    "\n",
    "def gelu_pytorch(z):\n",
    "    return     # bitte Code ergaenzen <---------------- [Luecke (19)]\n",
    "\n",
    "def hswish_pytorch(z):\n",
    "    return     # bitte Code ergaenzen <---------------- [Luecke (20)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Visualisierung & Überprüfung:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Funktionsverlauf visualisieren\n",
    "print(\"Sigmoid, Tanh, Softplus:\")\n",
    "plot(forward_functions=(sigmoid_forward,\n",
    "                        tanh_forward,\n",
    "                        softplus_forward),\n",
    "     backward_functions=(sigmoid_backward,\n",
    "                         tanh_backward,\n",
    "                         softplus_backward),\n",
    "     pytorch_functions=(sigmoid_pytorch,\n",
    "                        tanh_pytorch,\n",
    "                        softplus_pytorch),\n",
    "     labels=('Sigmoid', 'Tanh', 'Softplus'))\n",
    "\n",
    "\n",
    "print(f\"{'-'*80}\\nReLU, Leaky ReLU, ReLU-n, ELU:\")\n",
    "plot(forward_functions=(relu_forward,\n",
    "                        partial(relu_forward, alpha=0.01),\n",
    "                        partial(relu_forward, n=6),\n",
    "                        elu_forward),\n",
    "     backward_functions=(relu_backward,\n",
    "                         partial(relu_backward, alpha=0.01),\n",
    "                         partial(relu_backward, n=6),\n",
    "                         elu_backward),\n",
    "     pytorch_functions=(relu_pytorch,\n",
    "                        partial(relu_pytorch, alpha=0.01),\n",
    "                        partial(relu_pytorch, n=6),\n",
    "                        elu_pytorch),\n",
    "     labels=('ReLU', 'Leaky ReLU (alpha: 0.01)', 'ReLU-n (n: 6)', 'ELU'),\n",
    "     linestyles=('-', '--', ':', '-.'))\n",
    "\n",
    "\n",
    "print(f\"{'-'*80}\\nSwish, Swish-Beta, Hard-Swish, GELU:\")\n",
    "plot(forward_functions=(swish_forward,\n",
    "                        partial(swish_forward, beta=0.1),\n",
    "                        partial(swish_forward, beta=10.0),\n",
    "                        hswish_forward,\n",
    "                        gelu_forward),\n",
    "     backward_functions=(swish_backward,\n",
    "                        partial(swish_backward, beta=0.1),\n",
    "                        partial(swish_backward, beta=10.0),\n",
    "                        hswish_backward,\n",
    "                        gelu_backward),\n",
    "     pytorch_functions=(swish_pytorch,\n",
    "                        partial(swish_pytorch, beta=0.1),\n",
    "                        partial(swish_pytorch, beta=10.0),\n",
    "                        hswish_pytorch,\n",
    "                        gelu_pytorch),\n",
    "     labels=('Swish', 'Swish (beta: 0.1)', 'Swish (beta: 10.0)', 'Hard-Swish', 'GELU'),\n",
    "     linestyles=('-', '-', '-', '--')\n",
    "    )\n",
    "\n",
    "# Implementierung ueberpruefen\n",
    "interactive_check('pytorch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"border-width: 5px\">\n",
    "\n",
    "<a name=\"e\"></a>\n",
    "### (e) Nutzen Sie die realisierten Python-Implementierungen, um die Ergebnisse aus Teilaufgabe (d) zu reproduzieren.\n",
    "\n",
    "*Einstellungen für NumPy:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ausgabewerte für NumPy auf 4 Nachkommastellen runden\n",
    "np.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Definition der Eingabe, der Gewichte, der Biasgewichte und des Teachers:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eingabe\n",
    "x = np.array([-2, 2], dtype='float32')\n",
    "\n",
    "# Gewichte\n",
    "w = np.array([-1, 1], dtype='float32')\n",
    "\n",
    "# Teacher\n",
    "t = 0\n",
    "\n",
    "# Hyper-Parameter\n",
    "eta = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Reproduktion der Ergebnisse mit NumPy:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funktionen definieren\n",
    "functions = {\n",
    "    'Sigmoid': (sigmoid_forward,\n",
    "                sigmoid_backward),\n",
    "    'tanh': (tanh_forward,\n",
    "             tanh_backward),\n",
    "    'Softplus': (softplus_forward,\n",
    "                 softplus_backward),\n",
    "    'ReLU (alpha: 0.01, n: 6)': (partial(relu_forward, alpha=0.01, n=6),\n",
    "                                 partial(relu_backward, alpha=0.01, n=6)),\n",
    "    # PReLU hier ebenfalls durch ReLU realisiert\n",
    "    'PReLU (alpha: 0.01)': (partial(relu_forward, alpha=0.01, n=None),\n",
    "                            partial(relu_backward, alpha=0.01, n=None)),\n",
    "    'ELU (alpha: 1.0)': (partial(elu_forward, alpha=1),\n",
    "                         partial(elu_backward, alpha=1)),\n",
    "    'Swish (beta: 1.0)': (partial(swish_forward, beta=1),\n",
    "                         partial(swish_backward, beta=1))\n",
    "}\n",
    "\n",
    "# Gewichtsupdates fuer jede Funktion bestimmen\n",
    "for name, (forward, backward) in functions.items():\n",
    "    # Aktivierung `z` (ohne Bias) bestimmen\n",
    "    # bitte Code ergaenzen <---------------- [Luecke (21)]\n",
    "\n",
    "    # Ausgabe `y` bestimmen\n",
    "    # bitte Code ergaenzen <---------------- [Luecke (22)]\n",
    "\n",
    "    # Ableitung `dydz` bestimmen\n",
    "    # bitte Code ergaenzen <---------------- [Luecke (23)]\n",
    "\n",
    "    # Update `delta_w` bestimmen\n",
    "    # bitte Code ergaenzen <---------------- [Luecke (24)]\n",
    "\n",
    "    # Ergebnisse ausgeben\n",
    "    print(f\"{(name+':'):<25s} z={z.item():0.4f}, y={y.item():0.4f}, dydz={dydz.item():0.4f}, delta_w={delta_w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Was fällt bezüglich der Abweichung zum Teacher und bezüglich des Gradienten auf?\n",
    "<br>\n",
    "<br>\n",
    "<details>\n",
    "    <summary>&#9432; <i>Überprüfung &nbsp; &nbsp; <font color=\"CCCCCC\">(anklicken, um Lösung anzuzeigen)</font></i></summary>\n",
    "    <br>\n",
    "    <i>Augaben bei korrekter Implementierung:</i>\n",
    "    <br>\n",
    "    <code style=\"padding: 0\">\n",
    "Sigmoid:                  z=4.0000, y=0.9820, dydz=0.0177, delta_w=[ 0.0035 -0.0035]\n",
    "tanh:                     z=4.0000, y=0.9993, dydz=0.0013, delta_w=[ 0.0003 -0.0003]\n",
    "Softplus:                 z=4.0000, y=4.0181, dydz=0.9820, delta_w=[ 0.7892 -0.7892]\n",
    "ReLU (alpha: 0.01, n: 6): z=4.0000, y=4.0000, dydz=1.0000, delta_w=[ 0.8 -0.8]\n",
    "PReLU (alpha: 0.01):      z=4.0000, y=4.0000, dydz=1.0000, delta_w=[ 0.8 -0.8]\n",
    "ELU (alpha: 1.0):         z=4.0000, y=4.0000, dydz=1.0000, delta_w=[ 0.8 -0.8]\n",
    "Swish (beta: 1.0):        z=4.0000, y=3.9281, dydz=1.0527, delta_w=[ 0.827 -0.827]\n",
    "</code>\n",
    "    <br>\n",
    "    <br>\n",
    "    <i>Begründung:</i>\n",
    "    <ul>\n",
    "        <li>Die Ausgabe des Neurons weicht für alle Funktionen stark vom Teacher ab.</li>\n",
    "        <li>Sigmoid und Tanh befinden sich im gesättigten Bereich, daher verschwindender Gradient und keine zielführende Gewichtsänderung</li>\n",
    "    </ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Reproduktion der Ergebnisse mit PyTorch:*\n",
    "\n",
    "Während in NumPy die konkrete Updatevorschrift implementiert wurde, muss in der PyTorch-Implementierung lediglich der Vorwärtsprozess definiert werden. AutoGrad übernimmt für Sie die Bestimmung der Gradienten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eingabe, Gewichte, Biasgewichte und Teacher zu PyTorch konvertieren\n",
    "x_pytorch = torch.tensor(x)\n",
    "w_pytorch = torch.tensor(w, requires_grad=True)\n",
    "t_pytorch = torch.tensor(t)\n",
    "\n",
    "# Funktionen definieren\n",
    "functions_pytorch = {\n",
    "    'Sigmoid': sigmoid_pytorch,\n",
    "    'tanh': tanh_pytorch,\n",
    "    'Softplus': softplus_pytorch,\n",
    "    'ReLU (alpha: 0.01, n: 6)': partial(relu_pytorch, alpha=0.01, n=6),\n",
    "    # PReLU hier ebenfalls durch ReLU realisiert\n",
    "    'PReLU (alpha: 0.01)': partial(relu_pytorch, alpha=0.01, n=None),\n",
    "    'ELU (alpha: 1.0)': partial(elu_pytorch, alpha=1),\n",
    "    'Swish (beta: 1.0)': partial(swish_pytorch, beta=1)\n",
    "}\n",
    "\n",
    "# Gewichtsupdates fuer jede Funktion bestimmen\n",
    "for name, forward in functions_pytorch.items():\n",
    "    # Gradienten zuruecksetzen\n",
    "    w_pytorch.grad = None\n",
    "\n",
    "    # Aktivierung `z_pytorch` (ohne Bias) bestimmen\n",
    "    # bitte Code ergaenzen <---------------- [Luecke (25)]\n",
    "\n",
    "    # Gradienten zur Aktivierung speichern\n",
    "    z_pytorch.retain_grad()\n",
    "\n",
    "    # Ausgabe `y_pytorch` bestimmen\n",
    "    # bitte Code ergaenzen <---------------- [Luecke (26)]\n",
    "\n",
    "    # Gradienten zur Ausgabe ebenfalls speichern\n",
    "    y_pytorch.retain_grad()\n",
    "\n",
    "    # Fehler `loss` berechnen\n",
    "    # bitte Code ergaenzen <---------------- [Luecke (27)]\n",
    "\n",
    "    # AutoGrad: Backpropagation + Gradienten bestimmen\n",
    "    loss.backward()\n",
    "\n",
    "    # Update `delta_w_pytorch` bestimmen\n",
    "    # bitte Code ergaenzen <---------------- [Luecke (28)]\n",
    "\n",
    "    # Werte fuer Ausgabe bestimmen\n",
    "    # Beachten Sie, dass wir nur an dem Teilgradienten dydz interessiert sind.\n",
    "    # Dieser muss aus den Gradienten fuer y und z noch bestimmt werden\n",
    "    z = z_pytorch.detach().numpy()\n",
    "    y = y_pytorch.detach().numpy()\n",
    "    dydz = z_pytorch.grad.numpy() / y_pytorch.grad.numpy()\n",
    "    delta_w = delta_w_pytorch.detach().numpy()\n",
    "\n",
    "    # Ergebnisse ausgeben\n",
    "    print(f\"{(name+':'):<25s} z={z.item():0.4f}, y={y.item():0.4f}, dydz={dydz.item():0.4f}, delta_w={delta_w}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary>&#9432; <i>Überprüfung &nbsp; &nbsp; <font color=\"CCCCCC\">(anklicken, um Lösung anzuzeigen)</font></i></summary>\n",
    "    <br>\n",
    "    <i>Augaben bei korrekter Implementierung:</i>\n",
    "    <br>\n",
    "    <code style=\"padding: 0\">\n",
    "Sigmoid:                  z=4.0000, y=0.9820, dydz=0.0177, delta_w=[ 0.0035 -0.0035]\n",
    "tanh:                     z=4.0000, y=0.9993, dydz=0.0013, delta_w=[ 0.0003 -0.0003]\n",
    "Softplus:                 z=4.0000, y=4.0181, dydz=0.9820, delta_w=[ 0.7892 -0.7892]\n",
    "ReLU (alpha: 0.01, n: 6): z=4.0000, y=4.0000, dydz=1.0000, delta_w=[ 0.8 -0.8]\n",
    "PReLU (alpha: 0.01):      z=4.0000, y=4.0000, dydz=1.0000, delta_w=[ 0.8 -0.8]\n",
    "ELU (alpha: 1.0):         z=4.0000, y=4.0000, dydz=1.0000, delta_w=[ 0.8 -0.8]\n",
    "Swish (beta: 1.0):        z=4.0000, y=3.9281, dydz=1.0527, delta_w=[ 0.827 -0.827]\n",
    "</code>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$_{_\\text{Created for Deep Learning for Computer Vision (DL4CV)}}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}